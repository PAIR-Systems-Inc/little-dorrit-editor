{
  "model_name": "or_qwen_vl_max",
  "date": "2025-04-07T01:53:16.038412",
  "annotator": "Qwen VL Max",
  "annotation_date": "2025-04-06T00:50:13.544268",
  "details": [
    {
      "observed_edit_num": 0,
      "expected_edit_num": 0,
      "tp": 0.09999999999999998,
      "fp": 0.45,
      "fn": 0.45,
      "type": "punctuation",
      "original_text": "said",
      "corrected_text": "said,",
      "observed_line_number": 20,
      "line_diff": 3,
      "line_number_penalty": 0.9,
      "judgement": "The predicted edit has the correct type ('punctuation'), matching the ground truth. The core edit identified by the ground truth is adding a comma after the word 'said', changing 'said when' to 'said, when'. The prediction correctly captures this punctuation insertion\u2014adding a comma after 'said.' Even though the prediction provides less context ('said' \u2192 'said,') compared to the ground truth ('said when' \u2192 'said, when'), the critical essence of the change (addition of a comma after 'said') is accurately captured. Therefore, both edit type and text content align with the ground truth's intended correction."
    },
    {
      "observed_edit_num": 1,
      "expected_edit_num": null,
      "tp": 0.0,
      "fp": 1.0,
      "fn": 0.0,
      "type": "replacement",
      "original_text": "high-road",
      "corrected_text": "high-road",
      "observed_line_number": 1,
      "line_diff": null,
      "line_number_penalty": 0.0,
      "judgement": "False positive: no matching ground truth edit found"
    },
    {
      "observed_edit_num": 2,
      "expected_edit_num": null,
      "tp": 0.0,
      "fp": 1.0,
      "fn": 0.0,
      "type": "punctuation",
      "original_text": "costing",
      "corrected_text": "costing",
      "observed_line_number": 1,
      "line_diff": null,
      "line_number_penalty": 0.0,
      "judgement": "False positive: no matching ground truth edit found"
    },
    {
      "observed_edit_num": 3,
      "expected_edit_num": null,
      "tp": 0.0,
      "fp": 1.0,
      "fn": 0.0,
      "type": "punctuation",
      "original_text": "and",
      "corrected_text": "and,",
      "observed_line_number": 10,
      "line_diff": null,
      "line_number_penalty": 0.0,
      "judgement": "False positive: no matching ground truth edit found"
    },
    {
      "observed_edit_num": null,
      "expected_edit_num": 1,
      "tp": 0.0,
      "fp": 0.0,
      "fn": 1.0,
      "type": "punctuation",
      "original_text": "high road",
      "corrected_text": "high-road",
      "observed_line_number": null,
      "line_diff": null,
      "line_number_penalty": 0.0,
      "judgement": "False negative: ground truth edit not found in prediction"
    },
    {
      "observed_edit_num": null,
      "expected_edit_num": 2,
      "tp": 0.0,
      "fp": 0.0,
      "fn": 1.0,
      "type": "punctuation",
      "original_text": "association, did",
      "corrected_text": "association did",
      "observed_line_number": null,
      "line_diff": null,
      "line_number_penalty": 0.0,
      "judgement": "False negative: ground truth edit not found in prediction"
    },
    {
      "observed_edit_num": null,
      "expected_edit_num": 3,
      "tp": 0.0,
      "fp": 0.0,
      "fn": 1.0,
      "type": "punctuation",
      "original_text": "earnestly telling",
      "corrected_text": "earnestly, telling",
      "observed_line_number": null,
      "line_diff": null,
      "line_number_penalty": 0.0,
      "judgement": "False negative: ground truth edit not found in prediction"
    },
    {
      "observed_edit_num": null,
      "expected_edit_num": 4,
      "tp": 0.0,
      "fp": 0.0,
      "fn": 1.0,
      "type": "punctuation",
      "original_text": "said",
      "corrected_text": "said,",
      "observed_line_number": null,
      "line_diff": null,
      "line_number_penalty": 0.0,
      "judgement": "False negative: ground truth edit not found in prediction"
    },
    {
      "observed_edit_num": null,
      "expected_edit_num": 5,
      "tp": 0.0,
      "fp": 0.0,
      "fn": 1.0,
      "type": "punctuation",
      "original_text": "Clennam in",
      "corrected_text": "Clennam, in",
      "observed_line_number": null,
      "line_diff": null,
      "line_number_penalty": 0.0,
      "judgement": "False negative: ground truth edit not found in prediction"
    },
    {
      "observed_edit_num": null,
      "expected_edit_num": 6,
      "tp": 0.0,
      "fp": 0.0,
      "fn": 1.0,
      "type": "punctuation",
      "original_text": "materials costing",
      "corrected_text": "materials, costing",
      "observed_line_number": null,
      "line_diff": null,
      "line_number_penalty": 0.0,
      "judgement": "False negative: ground truth edit not found in prediction"
    },
    {
      "observed_edit_num": null,
      "expected_edit_num": 7,
      "tp": 0.0,
      "fp": 0.0,
      "fn": 1.0,
      "type": "punctuation",
      "original_text": "about and",
      "corrected_text": "about, and",
      "observed_line_number": null,
      "line_diff": null,
      "line_number_penalty": 0.0,
      "judgement": "False negative: ground truth edit not found in prediction"
    }
  ]
}