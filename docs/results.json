[
  {
    "model_name": "GPT-4o (2024-11-20)",
    "precision": 0.42857142857142855,
    "recall": 0.2777777777777778,
    "f1_score": 0.33707865168539325,
    "date": "2025-04-06T00:11:08.214843",
    "shots": 2,
    "config": {
      "model_id": "or_gpt_4o_2024_11_20",
      "display_name": "GPT-4o (2024-11-20)",
      "shots": 2,
      "temperature": 0.0,
      "date": "2025-04-05",
      "notes": "Benchmark run with 2-shot learning"
    },
    "details": {
      "precision": 0.42857142857142855,
      "recall": 0.2777777777777778,
      "f1_score": 0.33707865168539325,
      "by_type": {
        "capitalization": {
          "precision": 0.5166666666666667,
          "recall": 0.5166666666666667,
          "f1": 0.5166666666666667,
          "count": 6
        },
        "punctuation": {
          "precision": 0.31916666666666665,
          "recall": 0.195,
          "f1": 0.23845238095238094,
          "count": 40
        },
        "replacement": {
          "precision": 0.225,
          "recall": 0.225,
          "f1": 0.225,
          "count": 8
        },
        "insertion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        }
      },
      "correct_count": 15,
      "total_ground_truth": 54,
      "total_predicted": 35,
      "file_results": [
        {
          "model_name": "or_gpt_4o_2024_11_20",
          "precision": 0.8666666666666667,
          "recall": 0.52,
          "f1_score": 0.65,
          "date": "2025-04-06T00:04:02.035470",
          "details": {
            "precision": 0.8666666666666667,
            "recall": 0.52,
            "f1_score": 0.65,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.8666666666666667,
                "recall": 0.65,
                "f1": 0.7428571428571429,
                "count": 4
              }
            },
            "correct_count": 3,
            "total_ground_truth": 5,
            "total_predicted": 3,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit exactly matches the ground truth in terms of edit type ('punctuation'). It clearly captures the intended essential change from 'Fellow Travellers' to 'Fellow Travellers.' adding a period. Differences in page numbering or line numbers are explicitly ignored, so they do not impact this evaluation. Therefore, both the edit type and the text content match the ground truth perfectly, making the prediction correct.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit type ('punctuation') matches the ground truth exactly. Additionally, the core textual change, adding a comma after 'howling', precisely reflects the intention of the ground truth edit ('howling over' \u2192 'howling, over'). The prediction does provide additional context ('over yonder, to-day, sir'), but this does not contradict or obscure the essential punctuation edit indicated by the ground truth. Thus, both criteria, edit type accuracy and text content accuracy, are met.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the ground truth edit type ('punctuation'). Examining the text content accuracy, the ground truth corrects 'pockets and' to 'pockets, and', introducing a comma after 'pockets'. The predicted edit also correctly introduces the same punctuation correction ('pockets and rattling' becomes 'pockets, and rattling'). Additional contextual words ('rattling') in the prediction do not invalidate the accuracy, as the essential punctuation change aligns exactly with the ground truth intention. Therefore, the predicted edit successfully meets both criteria of edit type and core text content accuracy.",
                "is_correct_with_penalty": true,
                "score": 0.6,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "Fellow Travellers",
                  "corrected_text": "Fellow Travellers.",
                  "line_number": 0,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Fellow Travellers",
                  "corrected_text": "Fellow Travellers.",
                  "line_number": 0,
                  "page": "19",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "howling over",
                  "corrected_text": "howling, over",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "howling over yonder to-day, sir",
                  "corrected_text": "howling, over yonder, to-day, sir",
                  "line_number": 1,
                  "page": "19",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "pockets and",
                  "corrected_text": "pockets, and",
                  "line_number": 17,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "pockets and rattling",
                  "corrected_text": "pockets, and rattling",
                  "line_number": 15,
                  "page": "19",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "003"
        },
        {
          "model_name": "or_gpt_4o_2024_11_20",
          "precision": 0.275,
          "recall": 0.22000000000000003,
          "f1_score": 0.24444444444444446,
          "date": "2025-04-06T00:04:44.097244",
          "details": {
            "precision": 0.275,
            "recall": 0.22000000000000003,
            "f1_score": 0.24444444444444446,
            "by_type": {
              "punctuation": {
                "precision": 0.275,
                "recall": 0.275,
                "f1": 0.275,
                "count": 4
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 5,
            "total_predicted": 4,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the ground truth edit. Specifically, the edit type ('punctuation') matches exactly. Moreover, the text content accuracy criterion is satisfied, as both the original ('yonder to-day') and the corrected ('yonder, to-day') texts in the predicted edit exactly match the ground truth. Although the predicted page number ('19') differs from the ground truth ('003.png'), we explicitly ignore line numbers and page references in this evaluation as instructed. Therefore, the predicted edit correctly captures the intention of the original edit in terms of both the edit type and the textual content.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              },
              {
                "is_correct": true,
                "reasoning": "The prediction exactly matches the edit type 'punctuation' of the ground truth. The text content accurately captures the essential correction of adding a comma after the word 'pockets'. Although the predicted edit includes additional context ('and rattling'), it correctly encompasses the core change indicated in the ground truth ('pockets and' \u2192 'pockets, and'). Thus, both criteria (edit type and core text content) are accurately satisfied.",
                "is_correct_with_penalty": false,
                "score": 0.09999999999999998,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "yonder to-day",
                  "corrected_text": "yonder, to-day",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "yonder to-day",
                  "corrected_text": "yonder, to-day",
                  "line_number": 1,
                  "page": "19",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "pockets and",
                  "corrected_text": "pockets, and",
                  "line_number": 17,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "pockets and rattling",
                  "corrected_text": "pockets, and rattling",
                  "line_number": 14,
                  "page": "19",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "003"
        },
        {
          "model_name": "or_gpt_4o_2024_11_20",
          "precision": 0.45,
          "recall": 0.3,
          "f1_score": 0.36000000000000004,
          "date": "2025-04-06T00:05:14.822011",
          "details": {
            "precision": 0.45,
            "recall": 0.3,
            "f1_score": 0.36000000000000004,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "capitalization": {
                "precision": 0.9,
                "recall": 0.9,
                "f1": 0.9,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 3,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the ground truth edit type ('capitalization'). Although the predicted edit specifies only the first word ('why' to 'Why'), this precisely reflects the core capitalization change intended by the ground truth edit ('why did he dine' to 'Why did he dine'). The additional words in the original phrase are context; the actual core change is the capitalization of the first word only. Therefore, both the edit type and core text content are accurately captured.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "capitalization",
                  "original_text": "why did he dine",
                  "corrected_text": "Why did he dine",
                  "line_number": 28,
                  "page": "004.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "capitalization",
                  "original_text": "why",
                  "corrected_text": "Why",
                  "line_number": 27,
                  "page": "184",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "004"
        },
        {
          "model_name": "or_gpt_4o_2024_11_20",
          "precision": 0.3,
          "recall": 0.19999999999999998,
          "f1_score": 0.23999999999999996,
          "date": "2025-04-06T00:05:48.529242",
          "details": {
            "precision": 0.3,
            "recall": 0.19999999999999998,
            "f1_score": 0.23999999999999996,
            "by_type": {
              "capitalization": {
                "precision": 0.6,
                "recall": 0.6,
                "f1": 0.6,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              }
            },
            "correct_count": 1,
            "total_ground_truth": 3,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The edit type 'capitalization' exactly matches the ground truth. The predicted edit ('why' \u2192 'Why') correctly captures the core capitalization correction identified by the ground truth ('why did he dine' \u2192 'Why did he dine'). Although the ground truth provides additional words for context, the essential capitalization change ('why' to 'Why') is accurately reflected in the prediction. Thus, the prediction passes both criteria: exact matching of edit type and accurate capture of the core textual change.",
                "is_correct_with_penalty": true,
                "score": 0.6,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "capitalization",
                  "original_text": "why did he dine",
                  "corrected_text": "Why did he dine",
                  "line_number": 28,
                  "page": "004.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "capitalization",
                  "original_text": "why",
                  "corrected_text": "Why",
                  "line_number": 26,
                  "page": "184",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "004"
        },
        {
          "model_name": "or_gpt_4o_2024_11_20",
          "precision": 0.75,
          "recall": 0.75,
          "f1_score": 0.75,
          "date": "2025-04-06T00:06:42.038122",
          "details": {
            "precision": 0.75,
            "recall": 0.75,
            "f1_score": 0.75,
            "by_type": {
              "replacement": {
                "precision": 0.9,
                "recall": 0.9,
                "f1": 0.9,
                "count": 1
              },
              "capitalization": {
                "precision": 0.6,
                "recall": 0.6,
                "f1": 0.6,
                "count": 1
              }
            },
            "correct_count": 2,
            "total_ground_truth": 2,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit type ('capitalization') exactly matches the ground truth edit type ('capitalization'). Regarding text content accuracy, the ground truth changes 'if he' to 'If he', and the prediction changes 'if' to 'If'. Although the prediction does not explicitly include the second word ('he'), the core intention of the edit\u2014capitalizing the initial word\u2014is correctly identified and performed. Additional context or partial segments are acceptable as long as the essential edit is correctly captured. Thus, the prediction accurately reflects the core capitalization correction intended by the ground truth.",
                "is_correct_with_penalty": true,
                "score": 0.6,
                "line_number_penalty": 0.4,
                "line_diff": 2
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the ground truth edit type, as both edits are replacements. Regarding text content accuracy, the essential change 'said' \u2192 'asked' from the ground truth is fully captured in the predicted edit. Although the ground truth includes the following word ('Clennam') with the original text ('said Clennam'), the core edit was changing the verb 'said' to 'asked', and this core edit is accurately and sufficiently captured by the predicted edit. Thus, both criteria of edit type and essential textual content are met, making the predicted edit correct.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "capitalization",
                  "original_text": "if he",
                  "corrected_text": "If he",
                  "line_number": 36,
                  "page": "005.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "capitalization",
                  "original_text": "if",
                  "corrected_text": "If",
                  "line_number": 38,
                  "page": "209",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "replacement",
                  "original_text": "said Clennam",
                  "corrected_text": "asked Clennam",
                  "line_number": 39,
                  "page": "005.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "said",
                  "corrected_text": "asked",
                  "line_number": 40,
                  "page": "209",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "005"
        },
        {
          "model_name": "or_gpt_4o_2024_11_20",
          "precision": 0.95,
          "recall": 0.95,
          "f1_score": 0.9500000000000001,
          "date": "2025-04-06T00:07:26.972532",
          "details": {
            "precision": 0.95,
            "recall": 0.95,
            "f1_score": 0.9500000000000001,
            "by_type": {
              "capitalization": {
                "precision": 1.0,
                "recall": 1.0,
                "f1": 1.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.9,
                "recall": 0.9,
                "f1": 0.9,
                "count": 1
              }
            },
            "correct_count": 2,
            "total_ground_truth": 2,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The edit type matches exactly (capitalization). The predicted edit correctly reflects the core intention of the capitalization correction from lowercase \"if\" to uppercase \"If\". Although the ground truth has additional context (\"if he\"), the predicted edit clearly captures the essential edit accurately by correcting the capitalization of \"if\". Thus, both criteria (edit type and textual accuracy of essential correction) are satisfied.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit is correct. First, the edit type 'replacement' exactly matches the ground truth. Second, the core textual change the ground truth intended ('said' to 'asked') is accurately captured. Although the predicted version has less context (just 'said' \u2192 'asked') compared to the ground truth ('said Clennam' \u2192 'asked Clennam'), this still correctly identifies and precisely addresses the essential edit. Additional context around the edited words would be acceptable, but its absence does not diminish the accuracy of capturing the core intent.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "capitalization",
                  "original_text": "if he",
                  "corrected_text": "If he",
                  "line_number": 36,
                  "page": "005.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "capitalization",
                  "original_text": "if",
                  "corrected_text": "If",
                  "line_number": 36,
                  "page": "209",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "replacement",
                  "original_text": "said Clennam",
                  "corrected_text": "asked Clennam",
                  "line_number": 39,
                  "page": "005.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "said",
                  "corrected_text": "asked",
                  "line_number": 38,
                  "page": "209",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "005"
        },
        {
          "model_name": "or_gpt_4o_2024_11_20",
          "precision": 0.5,
          "recall": 0.25,
          "f1_score": 0.3333333333333333,
          "date": "2025-04-06T00:08:15.164700",
          "details": {
            "precision": 0.5,
            "recall": 0.25,
            "f1_score": 0.3333333333333333,
            "by_type": {
              "punctuation": {
                "precision": 0.5,
                "recall": 0.25,
                "f1": 0.3333333333333333,
                "count": 8
              }
            },
            "correct_count": 2,
            "total_ground_truth": 8,
            "total_predicted": 4,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the ground truth edit type ('punctuation') and text content ('high road' \u2192 'high-road'). Although the page numbers differ significantly, the instructions explicitly stated to ignore line numbers and page identifiers. Therefore, since both the type and essential text correction precisely align, the prediction accurately captures the intent of the ground truth.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              },
              {
                "is_correct": true,
                "reasoning": "The edit type in the prediction ('punctuation') exactly matches the ground truth edit type ('punctuation'). Additionally, the text content accuracy is correct because the prediction accurately captures the intended punctuation change ('about and' \u2192 'about, and'), matching the core edit described in the ground truth. Although differences in page identifiers and line numbers exist, instructions explicitly state to ignore such discrepancies. Thus, both edit type and essential text correction are accurately captured by the prediction.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              },
              {
                "is_correct": true,
                "reasoning": "The edit type specified in both the ground truth and the prediction is exactly the same ('punctuation'). Regarding text content, both the ground truth and prediction describe the same core punctuation correction, transforming 'said' into 'said,'. The text content modification matches precisely in intent and detail. Differences in line number and page identification are explicitly ignored according to the given instructions. As the edit type and the essential punctuation change have been accurately captured, the predicted edit is correct.",
                "is_correct_with_penalty": false,
                "score": 0.09999999999999998,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "high road",
                  "corrected_text": "high-road",
                  "line_number": 1,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "high road",
                  "corrected_text": "high-road",
                  "line_number": 1,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "about and",
                  "corrected_text": "about, and",
                  "line_number": 13,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "about and",
                  "corrected_text": "about, and",
                  "line_number": 12,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "said",
                  "corrected_text": "said,",
                  "line_number": 25,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "said",
                  "corrected_text": "said,",
                  "line_number": 28,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "006"
        },
        {
          "model_name": "or_gpt_4o_2024_11_20",
          "precision": 0.525,
          "recall": 0.2625,
          "f1_score": 0.35,
          "date": "2025-04-06T00:09:18.107086",
          "details": {
            "precision": 0.525,
            "recall": 0.2625,
            "f1_score": 0.35,
            "by_type": {
              "punctuation": {
                "precision": 0.525,
                "recall": 0.2625,
                "f1": 0.35,
                "count": 8
              }
            },
            "correct_count": 3,
            "total_ground_truth": 8,
            "total_predicted": 4,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The edit type matches (both 'punctuation'), but the text content is different. The ground truth edit shows the original text as 'high road' converted to 'high-road', whereas the prediction shows the original text as 'highroad' converted to 'high-road'. Because the original text differs ('high road' vs 'highroad'), the predicted edit does not match the ground truth's intended edit. Even though they result in the same corrected text 'high-road', the discrepancy in original text means the prediction does not accurately reflect the ground truth's intended edit.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the edit type exactly ('punctuation') and accurately captures the specific text change described by the ground truth ('about and' to 'about, and'). Line numbers were ignored as instructed, and despite differing page identifiers, the core punctuation correction matches exactly. Thus, both the edit type and text content criteria are fully satisfied.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit matches exactly in terms of edit type ('punctuation') and text content ('said' corrected to 'said,'). There is no discrepancy in the essential intended correction. The page number and line number differences are explicitly disregarded based on the evaluation criteria provided, leaving type and text content as the only factors for consideration. Thus, the predicted edit correctly captures the intention of the ground truth edit.",
                "is_correct_with_penalty": true,
                "score": 0.6,
                "line_number_penalty": 0.4,
                "line_diff": 2
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit matches the ground truth exactly in both criteria assessed. First, the edit type ('punctuation') is identical between prediction and ground truth. Additionally, the text content in the predicted edit ('Clennam in' corrected to 'Clennam, in') perfectly captures the essential punctuation change identified in the ground truth edit. Even though the prediction provides a different page number and line number, these specifics are explicitly ignored according to evaluation instructions. Thus, as both edit type and the core textual correction are fully accurate, the prediction is correct.",
                "is_correct_with_penalty": true,
                "score": 0.6,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "high road",
                  "corrected_text": "high-road",
                  "line_number": 1,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "highroad",
                  "corrected_text": "high-road",
                  "line_number": 1,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "about and",
                  "corrected_text": "about, and",
                  "line_number": 13,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "about and",
                  "corrected_text": "about, and",
                  "line_number": 12,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "said",
                  "corrected_text": "said,",
                  "line_number": 25,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "said",
                  "corrected_text": "said,",
                  "line_number": 27,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "Clennam in",
                  "corrected_text": "Clennam, in",
                  "line_number": 29,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Clennam in",
                  "corrected_text": "Clennam, in",
                  "line_number": 31,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "006"
        },
        {
          "model_name": "or_gpt_4o_2024_11_20",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T00:09:51.079913",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The edit type 'punctuation' matches exactly with the ground truth. However, the original text in the ground truth is 'Sun and Shadow' (with no punctuation), whereas the prediction lists the original text with a comma 'Sun and Shadow,'. This discrepancy in the original text indicates that the predicted edit does not properly identify the exact punctuation change from 'Sun and Shadow' to 'Sun and Shadow.'. Thus, although the edit type is correctly identified, the essential textual content does not accurately match the ground truth edit's intention.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "007.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow,",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "007"
        },
        {
          "model_name": "or_gpt_4o_2024_11_20",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T00:10:29.215513",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "Although the edit type ('punctuation') exactly matches the ground truth, the predicted edit incorrectly identifies the original text as 'Sun and Shadow,' instead of 'Sun and Shadow'. The ground truth shows the original as lacking punctuation (no comma), and the correct addition is just a period ('Sun and Shadow' \u2192 'Sun and Shadow.'). However, the predicted edit wrongly suggests the original text contained a comma ('Sun and Shadow,') being changed to a period ('Sun and Shadow.'). This differs significantly from the core textual correction intended by the ground truth, thus making the predicted edit incorrect.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "007.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow,",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "007"
        },
        {
          "model_name": "or_gpt_4o_2024_11_20",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T00:10:49.917361",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 5,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        },
        {
          "model_name": "or_gpt_4o_2024_11_20",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T00:11:08.214843",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 5,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        }
      ]
    }
  },
  {
    "model_name": "ChatGPT-4o",
    "precision": 0.2857142857142857,
    "recall": 0.18518518518518517,
    "f1_score": 0.2247191011235955,
    "date": "2025-04-06T00:19:05.317210",
    "shots": 2,
    "config": {
      "model_id": "or_chatgpt_4o_latest",
      "display_name": "ChatGPT-4o",
      "shots": 2,
      "temperature": 0.0,
      "date": "2025-04-05",
      "notes": "Benchmark run with 2-shot learning"
    },
    "details": {
      "precision": 0.2857142857142857,
      "recall": 0.18518518518518517,
      "f1_score": 0.2247191011235955,
      "by_type": {
        "punctuation": {
          "precision": 0.32833333333333325,
          "recall": 0.16249999999999998,
          "f1": 0.2097050865800866,
          "count": 40
        },
        "capitalization": {
          "precision": 0.48333333333333334,
          "recall": 0.48333333333333334,
          "f1": 0.48333333333333334,
          "count": 6
        },
        "replacement": {
          "precision": 0.024999999999999994,
          "recall": 0.024999999999999994,
          "f1": 0.024999999999999994,
          "count": 8
        },
        "insertion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        }
      },
      "correct_count": 10,
      "total_ground_truth": 54,
      "total_predicted": 35,
      "file_results": [
        {
          "model_name": "or_chatgpt_4o_latest",
          "precision": 0.6666666666666666,
          "recall": 0.4,
          "f1_score": 0.5,
          "date": "2025-04-06T00:12:43.492844",
          "details": {
            "precision": 0.6666666666666666,
            "recall": 0.4,
            "f1_score": 0.5,
            "by_type": {
              "punctuation": {
                "precision": 0.5,
                "recall": 0.25,
                "f1": 0.3333333333333333,
                "count": 4
              },
              "capitalization": {
                "precision": 1.0,
                "recall": 1.0,
                "f1": 1.0,
                "count": 1
              }
            },
            "correct_count": 2,
            "total_ground_truth": 5,
            "total_predicted": 3,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit type matches exactly, as both ground truth and prediction use 'punctuation'. The core punctuation change ('howling over' \u2192 'howling, over') is correctly captured. Although the prediction includes additional contextual words ('yonder to-day, sir'), these do not affect correctness according to the instructions, as extra context beyond the essential edit does not invalidate the prediction. Thus, the prediction accurately captures the ground truth intention.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the edit type from 'capitalization' to 'capitalization' exactly. Additionally, the original text ('Sir') and corrected text ('sir') precisely match the ground truth's core indicated edit. Although there is a discrepancy in the page identifier ('003.png' vs. 'page_19'), this evaluation explicitly instructs ignoring line numbers and focuses solely on edit type and text content. Therefore, the prediction accurately captures the intended editorial correction.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "howling over",
                  "corrected_text": "howling, over",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "howling over yonder to-day, Sir",
                  "corrected_text": "howling, over yonder to-day, sir",
                  "line_number": 1,
                  "page": "page_19",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "capitalization",
                  "original_text": "Sir",
                  "corrected_text": "sir",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "capitalization",
                  "original_text": "Sir",
                  "corrected_text": "sir",
                  "line_number": 1,
                  "page": "page_19",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "003"
        },
        {
          "model_name": "or_chatgpt_4o_latest",
          "precision": 0.5,
          "recall": 0.4,
          "f1_score": 0.4444444444444445,
          "date": "2025-04-06T00:13:36.366601",
          "details": {
            "precision": 0.5,
            "recall": 0.4,
            "f1_score": 0.4444444444444445,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.5,
                "recall": 0.5,
                "f1": 0.5,
                "count": 4
              }
            },
            "correct_count": 2,
            "total_ground_truth": 5,
            "total_predicted": 4,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit accurately matches the ground truth edit in both criteria evaluated. Specifically: 1) Edit Type Accuracy: Both are identified as \"punctuation\", correctly matching. 2) Text Content Accuracy: Both the original and corrected texts (\"howling over\" \u2192 \"howling, over\") are exactly matched. The differences in page numbering (\"003.png\" vs \"page_19\") are irrelevant because the evaluation instructions explicitly instruct to ignore line numbers or page references. Therefore, the predicted edit fully captures the intention of the ground truth edit.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              },
              {
                "is_correct": true,
                "reasoning": "The edit type ('punctuation') matches exactly between the ground truth and predicted edit. The textual content shown in both ('yonder to-day' corrected to 'yonder, to-day') is identical, correctly showing the addition of a comma as intended by the ground truth edit. Although the 'page' field differs ('003.png' vs. 'page_19'), the instruction explicitly states to ignore line numbers (and by extension related meta-data) for this evaluation. Thus, based solely on edit type and textual content accuracy, the predicted edit correctly captures the intention that the ground truth conveys.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "howling over",
                  "corrected_text": "howling, over",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "howling over",
                  "corrected_text": "howling, over",
                  "line_number": 1,
                  "page": "page_19",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "yonder to-day",
                  "corrected_text": "yonder, to-day",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "yonder to-day",
                  "corrected_text": "yonder, to-day",
                  "line_number": 1,
                  "page": "page_19",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "003"
        },
        {
          "model_name": "or_chatgpt_4o_latest",
          "precision": 0.45,
          "recall": 0.3,
          "f1_score": 0.36000000000000004,
          "date": "2025-04-06T00:14:10.604584",
          "details": {
            "precision": 0.45,
            "recall": 0.3,
            "f1_score": 0.36000000000000004,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "capitalization": {
                "precision": 0.9,
                "recall": 0.9,
                "f1": 0.9,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 3,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly identifies the edit type ('capitalization'), matching exactly the ground truth edit. Regarding text content, although the prediction focuses specifically on 'why' \u2192 'Why' and the ground truth shows a longer text 'why did he dine' \u2192 'Why did he dine', the predicted edit accurately captures the core intention of the original edit, which is simply capitalizing the first letter of 'why'. Additional text is contextually surrounding the main edit, and since the evaluation explicitly states that additional context around the core edit is acceptable, the prediction clearly meets both criteria. Therefore, the prediction is correct.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "capitalization",
                  "original_text": "why did he dine",
                  "corrected_text": "Why did he dine",
                  "line_number": 28,
                  "page": "004.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "capitalization",
                  "original_text": "why",
                  "corrected_text": "Why",
                  "line_number": 27,
                  "page": "184",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "004"
        },
        {
          "model_name": "or_chatgpt_4o_latest",
          "precision": 0.45,
          "recall": 0.3,
          "f1_score": 0.36000000000000004,
          "date": "2025-04-06T00:14:33.757547",
          "details": {
            "precision": 0.45,
            "recall": 0.3,
            "f1_score": 0.36000000000000004,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "capitalization": {
                "precision": 0.9,
                "recall": 0.9,
                "f1": 0.9,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 3,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly identifies the edit type as 'capitalization', matching the ground truth exactly. It precisely captures the core textual correction\u2014in this case, changing 'why' to 'Why'. Although the prediction only shows 'why' instead of the full 'why did he dine', the essential capitalization change is accurately represented because the critical edit from lowercase 'why' to capitalized 'Why' was correctly identified. Additional context or complete phrasing around the corrected word is not mandatory as per given criteria. Hence, the prediction accurately captures the intention of the ground truth edit.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "capitalization",
                  "original_text": "why did he dine",
                  "corrected_text": "Why did he dine",
                  "line_number": 28,
                  "page": "004.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "capitalization",
                  "original_text": "why",
                  "corrected_text": "Why",
                  "line_number": 27,
                  "page": "184",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "004"
        },
        {
          "model_name": "or_chatgpt_4o_latest",
          "precision": 0.09999999999999998,
          "recall": 0.09999999999999998,
          "f1_score": 0.09999999999999998,
          "date": "2025-04-06T00:15:12.990437",
          "details": {
            "precision": 0.09999999999999998,
            "recall": 0.09999999999999998,
            "f1_score": 0.09999999999999998,
            "by_type": {
              "capitalization": {
                "precision": 0.09999999999999998,
                "recall": 0.09999999999999998,
                "f1": 0.09999999999999998,
                "count": 1
              },
              "replacement": {
                "precision": 0.09999999999999998,
                "recall": 0.09999999999999998,
                "f1": 0.09999999999999998,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the type of edit (capitalization) specified by the ground truth exactly. The actual change in capitalization\u2014altering 'if' to 'If'\u2014is precisely captured. Although the ground truth includes slightly more context ('if he' \u2192 'If he'), the predicted edit adequately identifies the essential core change ('if' \u2192 'If'). Therefore, both the edit type and the core textual change accurately reflect the ground truth intention.",
                "is_correct_with_penalty": false,
                "score": 0.09999999999999998,
                "line_number_penalty": 0.9,
                "line_diff": 3
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly identifies the type ('replacement') matching exactly with the ground truth type. The essential textual correction ('said' to 'asked') precisely captures the core intention stated by the ground truth ('said Clennam' \u2192 'asked Clennam'). Although the ground truth included one additional word ('Clennam'), the predicted edit omits it but still clearly denotes the central correction: changing 'said' to 'asked'. According to the evaluation criteria provided, including fewer or more words as context is acceptable as long as the core edit is accurately identified. Thus, both edit type and core textual intent were accurately captured, making the prediction correct.",
                "is_correct_with_penalty": false,
                "score": 0.09999999999999998,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "capitalization",
                  "original_text": "if he",
                  "corrected_text": "If he",
                  "line_number": 36,
                  "page": "005.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "capitalization",
                  "original_text": "if",
                  "corrected_text": "If",
                  "line_number": 33,
                  "page": "209",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "replacement",
                  "original_text": "said Clennam",
                  "corrected_text": "asked Clennam",
                  "line_number": 39,
                  "page": "005.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "said",
                  "corrected_text": "asked",
                  "line_number": 36,
                  "page": "209",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "005"
        },
        {
          "model_name": "or_chatgpt_4o_latest",
          "precision": 0.04999999999999999,
          "recall": 0.04999999999999999,
          "f1_score": 0.04999999999999999,
          "date": "2025-04-06T00:15:41.117927",
          "details": {
            "precision": 0.04999999999999999,
            "recall": 0.04999999999999999,
            "f1_score": 0.04999999999999999,
            "by_type": {
              "replacement": {
                "precision": 0.09999999999999998,
                "recall": 0.09999999999999998,
                "f1": 0.09999999999999998,
                "count": 1
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit matches exactly in type ('replacement') and correctly captures the core textual change from 'said' to 'asked'. Although the prediction does not explicitly include the context word 'Clennam', it still accurately identifies the essential correction intended by the ground truth ('said' to 'asked'). Thus, both the edit type and the core text content change align correctly.",
                "is_correct_with_penalty": false,
                "score": 0.09999999999999998,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "replacement",
                  "original_text": "said Clennam",
                  "corrected_text": "asked Clennam",
                  "line_number": 39,
                  "page": "005.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "said",
                  "corrected_text": "asked",
                  "line_number": 36,
                  "page": "209",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "005"
        },
        {
          "model_name": "or_chatgpt_4o_latest",
          "precision": 0.25,
          "recall": 0.125,
          "f1_score": 0.16666666666666666,
          "date": "2025-04-06T00:16:32.916910",
          "details": {
            "precision": 0.25,
            "recall": 0.125,
            "f1_score": 0.16666666666666666,
            "by_type": {
              "punctuation": {
                "precision": 0.3333333333333333,
                "recall": 0.125,
                "f1": 0.18181818181818182,
                "count": 8
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 1,
            "total_ground_truth": 8,
            "total_predicted": 4,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The edit type in both ground truth and predicted edit is identical ('punctuation'). The text content also matches exactly, clearly capturing the essential punctuation insertion intended by the ground truth ('about and' \u2192 'about, and'). Differences in line numbers and page numbers were explicitly ignored per instructions, so these discrepancies do not factor into the evaluation. Thus, the prediction accurately captures the core intention of the edit.",
                "is_correct_with_penalty": false,
                "score": 0.09999999999999998,
                "line_number_penalty": 0.9,
                "line_diff": 3
              },
              {
                "is_correct": false,
                "reasoning": "The edit type 'punctuation' matches correctly between predicted and ground truth edits. However, the text content accuracy fails because the ground truth edit explicitly changes 'said when' into 'said, when', indicating the comma insertion occurs within a two-word phrase. Conversely, the prediction identifies only a single word 'said' changed to 'said,', omitting the critical contextual element, 'when'. Therefore, the predicted edit does not fully capture the essential context established by the ground truth edit.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.1,
                "line_diff": 1
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit type ('punctuation') matches exactly with the ground truth edit type ('punctuation'). The original and corrected texts are precisely the same ('Clennam in' \u2192 'Clennam, in'), indicating that the core correction of adding a comma after 'Clennam' is accurately captured. Although there is a discrepancy in 'page' information, the instructions clearly state to ignore line numbers (and by extension, page references), thus only the edit type and text content are evaluated. Based on these criteria, the prediction correctly reflects the intention of the ground truth edit.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "about and",
                  "corrected_text": "about, and",
                  "line_number": 13,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "about and",
                  "corrected_text": "about, and",
                  "line_number": 10,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "said when",
                  "corrected_text": "said, when",
                  "line_number": 23,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "said",
                  "corrected_text": "said,",
                  "line_number": 24,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "Clennam in",
                  "corrected_text": "Clennam, in",
                  "line_number": 29,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Clennam in",
                  "corrected_text": "Clennam, in",
                  "line_number": 28,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "006"
        },
        {
          "model_name": "or_chatgpt_4o_latest",
          "precision": 0.45,
          "recall": 0.225,
          "f1_score": 0.3,
          "date": "2025-04-06T00:17:05.439826",
          "details": {
            "precision": 0.45,
            "recall": 0.225,
            "f1_score": 0.3,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.6,
                "recall": 0.225,
                "f1": 0.3272727272727273,
                "count": 8
              }
            },
            "correct_count": 2,
            "total_ground_truth": 8,
            "total_predicted": 4,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The edit type matches exactly ('punctuation') in both the predicted and ground truth edits. Regarding text content accuracy, the predicted edit captures the core punctuation change indicated by the ground truth edit. The ground truth corrects 'said when' to 'said, when', explicitly showing a comma insertion after 'said'. The prediction similarly indicates 'said' corrected to 'said,', accurately reflecting the essential punctuation change intended in the original correction. Thus, despite minor differences in context (omission of the following word 'when'), the critical correction\u2014adding a comma after 'said'\u2014is correctly captured.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              },
              {
                "is_correct": true,
                "reasoning": "The edit type matches exactly ('punctuation' in both cases). The textual change accurately captures the ground truth correction ('Clennam in' to 'Clennam, in'). Line numbers and page identifiers are ignored according to evaluation instructions. Thus, both edit type and essential text content align precisely, making the prediction correct.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "said when",
                  "corrected_text": "said, when",
                  "line_number": 23,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "said",
                  "corrected_text": "said,",
                  "line_number": 24,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "Clennam in",
                  "corrected_text": "Clennam, in",
                  "line_number": 29,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Clennam in",
                  "corrected_text": "Clennam, in",
                  "line_number": 28,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "006"
        },
        {
          "model_name": "or_chatgpt_4o_latest",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T00:17:31.610507",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "Although both the ground truth and predicted edits share the same edit type ('punctuation') and have the identical corrected text ('Sun and Shadow.'), they differ in the original text: the ground truth is correcting 'Sun and Shadow' (no punctuation) to 'Sun and Shadow.', whereas the prediction is correcting 'Sun and Shadow,' (with a comma) to 'Sun and Shadow.'. This discrepancy indicates that the predicted edit incorrectly captures the original text before the change. According to the provided evaluation criteria, the prediction must correctly capture the core change identified by the ground truth. Since the original text in the prediction does not match the ground truth's original text, the prediction is not accurate.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "007.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow,",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "003",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "007"
        },
        {
          "model_name": "or_chatgpt_4o_latest",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T00:17:57.811271",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "While the predicted edit matches the edit type ('punctuation'), the text content accuracy is incorrect because the original text for the ground truth edit is 'Sun and Shadow', whereas the predicted edit incorrectly indicates the original text as 'Sun and Shadow,'. The core intended edit is simply adding a period at the end of 'Sun and Shadow', not replacing a comma with a period. Since the prediction inaccurately captures the original state of the text, the prediction does not correctly reflect the ground truth's intended correction.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "007.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow,",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "003",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "007"
        },
        {
          "model_name": "or_chatgpt_4o_latest",
          "precision": 0.12,
          "recall": 0.075,
          "f1_score": 0.0923076923076923,
          "date": "2025-04-06T00:18:31.886818",
          "details": {
            "precision": 0.12,
            "recall": 0.075,
            "f1_score": 0.0923076923076923,
            "by_type": {
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "punctuation": {
                "precision": 0.3,
                "recall": 0.12,
                "f1": 0.17142857142857143,
                "count": 5
              }
            },
            "correct_count": 1,
            "total_ground_truth": 8,
            "total_predicted": 5,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "Although the edit type ('punctuation') matches, the predicted edit does not accurately capture the essential text correction of the ground truth. The ground truth correction changes 'poplar trees' to 'poplar-trees,', adding both a hyphen between words and a comma punctuation. However, the predicted edit only modifies 'trees' and adds an em dash ('\u2014') after it, omitting the hyphenation between words and using a completely different punctuation. Thus, it fails to reflect the core intended correction ('poplar trees' \u2192 'poplar-trees,').",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.4,
                "line_diff": 2
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit matches exactly the edit type ('punctuation'), which is correct. Furthermore, the text content change indicated by the prediction ('man slowly' \u2192 'man, slowly') precisely matches the ground truth correction. Although the line number and page differ, the instructions explicitly advise ignoring line numbers. Therefore, the prediction correctly captures both the edit type and the essential textual punctuation edit of the ground truth.",
                "is_correct_with_penalty": true,
                "score": 0.6,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees,",
                  "line_number": 6,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "trees",
                  "corrected_text": "trees\u2014",
                  "line_number": 4,
                  "page": "137",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "man slowly",
                  "corrected_text": "man, slowly",
                  "line_number": 9,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "man slowly",
                  "corrected_text": "man, slowly",
                  "line_number": 7,
                  "page": "137",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        },
        {
          "model_name": "or_chatgpt_4o_latest",
          "precision": 0.019999999999999997,
          "recall": 0.012499999999999997,
          "f1_score": 0.01538461538461538,
          "date": "2025-04-06T00:19:05.317210",
          "details": {
            "precision": 0.019999999999999997,
            "recall": 0.012499999999999997,
            "f1_score": 0.01538461538461538,
            "by_type": {
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "punctuation": {
                "precision": 0.033333333333333326,
                "recall": 0.019999999999999997,
                "f1": 0.024999999999999994,
                "count": 5
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 5,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The predicted edit correctly matches the edit type as 'punctuation' and captures part of the ground truth's edit ('poplar trees' to 'poplar-trees'). However, it misses an essential punctuation element\u2014the addition of the comma at the end, present in the ground truth's edit ('poplar-trees,'). As capturing the complete core correction is critical, this omission makes the prediction incorrect.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.4,
                "line_diff": 2
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches both the edit type ('punctuation') and the text content ('man slowly' \u2192 'man, slowly'). The core punctuation change adding a comma is exactly captured. Differences in page numbers or line numbers are ignored as per evaluation instructions, so the prediction is fully correct.",
                "is_correct_with_penalty": false,
                "score": 0.09999999999999998,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees,",
                  "line_number": 6,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees",
                  "line_number": 4,
                  "page": "137",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "man slowly",
                  "corrected_text": "man, slowly",
                  "line_number": 9,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "man slowly",
                  "corrected_text": "man, slowly",
                  "line_number": 6,
                  "page": "137",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        }
      ]
    }
  },
  {
    "model_name": "Mistral Small 3.1 24B",
    "precision": 0.14285714285714285,
    "recall": 0.14,
    "f1_score": 0.1414141414141414,
    "date": "2025-04-06T08:48:58.370012",
    "shots": 2,
    "config": {
      "model_id": "or_mistral_small_3_1_24b_instruct",
      "display_name": "Mistral Small 3.1 24B",
      "shots": 2,
      "temperature": 0.0,
      "date": "2025-04-06",
      "notes": "Benchmark run with 2-shot learning"
    },
    "details": {
      "precision": 0.14285714285714285,
      "recall": 0.14,
      "f1_score": 0.1414141414141414,
      "by_type": {
        "insertion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        },
        "capitalization": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 6
        },
        "punctuation": {
          "precision": 0.125,
          "recall": 0.125,
          "f1": 0.125,
          "count": 32
        },
        "replacement": {
          "precision": 0.525,
          "recall": 0.26666666666666666,
          "f1": 0.34500000000000003,
          "count": 12
        },
        "deletion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        }
      },
      "correct_count": 7,
      "total_ground_truth": 50,
      "total_predicted": 49,
      "file_results": [
        {
          "model_name": "mistralai/mistral-small-3.1-24b-instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T02:47:39.894777",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 4,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_mistral_small_3_1_24b_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:44:55.907835",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_mistral_small_3_1_24b_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:45:10.899664",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "mistralai/mistral-small-3.1-24b-instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T02:47:57.392397",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_mistral_small_3_1_24b_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:45:25.631914",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_mistral_small_3_1_24b_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:45:37.457073",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 4,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_mistral_small_3_1_24b_instruct",
          "precision": 0.1111111111111111,
          "recall": 0.125,
          "f1_score": 0.11764705882352941,
          "date": "2025-04-06T08:46:15.373328",
          "details": {
            "precision": 0.1111111111111111,
            "recall": 0.125,
            "f1_score": 0.11764705882352941,
            "by_type": {
              "punctuation": {
                "precision": 0.125,
                "recall": 0.125,
                "f1": 0.125,
                "count": 8
              },
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 1,
            "total_ground_truth": 8,
            "total_predicted": 9,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "Edit type matches correctly as both ground truth and predicted edit specify 'punctuation'. However, the original text differs significantly. The ground truth shows 'high road' changed to 'high-road', indicating that the edit involves inserting punctuation (a hyphen) between two separate words ('high' and 'road'). However, the prediction indicates 'highroad' changed to 'high-road', suggesting the original text was already a single word merged ('highroad'). Thus, the nature of the error corrected significantly differs: spelling/word separation vs. merely punctuation. Because the original text context differs fundamentally in its intention and implications, the predicted edit does not capture the core textual change accurately.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.1,
                "line_diff": 1
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit matches the ground truth edit exactly in terms of edit type ('punctuation') and captures the exact intended change ('about and' \u2192 'about, and'). Since line numbers and page details are explicitly ignored for this evaluation and the core punctuation change is correctly represented, the prediction accurately reflects the ground truth's intention.",
                "is_correct_with_penalty": false,
                "score": 0.09999999999999998,
                "line_number_penalty": 0.9,
                "line_diff": 3
              },
              {
                "is_correct": true,
                "reasoning": "The edit type matches exactly as 'punctuation'. The text content accuracy criterion is also fully met: the original text 'Clennam in' and corrected text 'Clennam, in' are identical in both ground truth and prediction. Although the page numbers differ, the instruction clearly states to ignore line numbers and context like page numbers. Therefore, the essential punctuation edit captured by the prediction precisely matches the ground truth's intended edit.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "high road",
                  "corrected_text": "high-road",
                  "line_number": 1,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "highroad",
                  "corrected_text": "high-road",
                  "line_number": 0,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "about and",
                  "corrected_text": "about, and",
                  "line_number": 13,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "about and",
                  "corrected_text": "about, and",
                  "line_number": 10,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "Clennam in",
                  "corrected_text": "Clennam, in",
                  "line_number": 29,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Clennam in",
                  "corrected_text": "Clennam, in",
                  "line_number": 28,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "006"
        },
        {
          "model_name": "mistralai/mistral-small-3.1-24b-instruct",
          "precision": 1.0,
          "recall": 1.0,
          "f1_score": 1.0,
          "date": "2025-04-05T02:48:20.324994",
          "details": {
            "precision": 1.0,
            "recall": 1.0,
            "f1_score": 1.0,
            "by_type": {
              "punctuation": {
                "precision": 1.0,
                "recall": 1.0,
                "f1": 1.0,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the edit type ('punctuation') exactly as the ground truth. Additionally, the content change from 'Sun and Shadow' to 'Sun and Shadow.' precisely matches the essential punctuation correction indicated in the ground truth. Although the predicted edit references a different page ('003.png' instead of '007.png'), page numbers and line numbers were explicitly instructed to be ignored for the evaluation. Therefore, since both the edit type and text content accurately reflect the ground truth intention, the prediction is correct.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "007.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "007"
        },
        {
          "model_name": "or_mistral_small_3_1_24b_instruct",
          "precision": 1.0,
          "recall": 1.0,
          "f1_score": 1.0,
          "date": "2025-04-06T08:46:40.450935",
          "details": {
            "precision": 1.0,
            "recall": 1.0,
            "f1_score": 1.0,
            "by_type": {
              "punctuation": {
                "precision": 1.0,
                "recall": 1.0,
                "f1": 1.0,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit type 'punctuation' exactly matches the ground truth edit type 'punctuation', so the Edit Type Accuracy criterion is met. Additionally, the core textual correction indicated by both edits is accurately captured: the addition of a period after the word 'Shadow'. Although the ground truth includes additional preceding context ('Sun and'), the essential intention\u2014to append punctuation ('.') at the end of the phrase\u2014remains the same. Thus, the prediction correctly captures the intended edit.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "007.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Shadow",
                  "corrected_text": "Shadow.",
                  "line_number": 0,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "007"
        },
        {
          "model_name": "or_mistral_small_3_1_24b_instruct",
          "precision": 1.0,
          "recall": 1.0,
          "f1_score": 1.0,
          "date": "2025-04-06T08:47:11.876170",
          "details": {
            "precision": 1.0,
            "recall": 1.0,
            "f1_score": 1.0,
            "by_type": {
              "punctuation": {
                "precision": 1.0,
                "recall": 1.0,
                "f1": 1.0,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit type ('punctuation') matches exactly with the ground truth edit type. The text content alteration ('Sun and Shadow' changed to 'Sun and Shadow.') also matches the ground truth change precisely. The discrepancy in the page number ('007.png' in ground truth vs. '003.png' in predicted) is not relevant to the evaluation criteria provided, as the evaluation guidelines explicitly ask to ignore line numbers (and similarly implicitly page numbers), focusing solely on edit type and text content accuracy. Therefore, the predicted edit correctly captures the intention of the ground truth edit.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "007.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "007"
        },
        {
          "model_name": "mistralai/mistral-small-3.1-24b-instruct",
          "precision": 0.15714285714285717,
          "recall": 0.1375,
          "f1_score": 0.14666666666666667,
          "date": "2025-04-05T02:49:06.346515",
          "details": {
            "precision": 0.15714285714285717,
            "recall": 0.1375,
            "f1_score": 0.14666666666666667,
            "by_type": {
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "replacement": {
                "precision": 0.55,
                "recall": 0.3666666666666667,
                "f1": 0.44,
                "count": 3
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 1,
            "total_ground_truth": 8,
            "total_predicted": 7,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit accurately matches the ground truth. The edit type ('replacement') is identical in both predictions. The original text ('Saone') and the corrected text ('Sa\u00f4ne') match exactly, clearly capturing the core textual correction intended by the ground truth edit. Page number differences and line numbers were explicitly stated to be ignored. Therefore, the predicted edit correctly represents the intended handwritten editorial correction.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              },
              {
                "is_correct": false,
                "reasoning": "The predicted edit correctly identifies the edit type as 'punctuation', matching the ground truth's edit type. It also accurately captures the hyphenation ('poplar trees' to 'poplar-trees'). However, it fails to include the comma that the ground truth adds after 'poplar-trees'. Since the comma is explicitly indicated in the ground truth correction ('poplar-trees,'), omitting it constitutes an incomplete capture of the intended punctuation edit. Therefore, the predicted edit does not fully match the essential text content specified by the ground truth.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.9,
                "line_diff": 3
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit has the exact matching edit type ('replacement') as the ground truth. It also precisely captures the intended text correction ('Saone' \u2192 'Sa\u00f4ne') without adding unnecessary changes or missing the core edit. Even though the predicted edit references a different line number and page, the instructions explicitly state to ignore line numbers, focusing solely on the accuracy of edit type and text content. Therefore, since both criteria are fully satisfied, the predicted edit is correct.",
                "is_correct_with_penalty": false,
                "score": 0.09999999999999998,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "replacement",
                  "original_text": "Saone",
                  "corrected_text": "Sa\u00f4ne",
                  "line_number": 1,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "Saone",
                  "corrected_text": "Sa\u00f4ne",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees,",
                  "line_number": 6,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees",
                  "line_number": 3,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "replacement",
                  "original_text": "Saone",
                  "corrected_text": "Sa\u00f4ne",
                  "line_number": 7,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "Saone",
                  "corrected_text": "Sa\u00f4ne",
                  "line_number": 4,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        },
        {
          "model_name": "or_mistral_small_3_1_24b_instruct",
          "precision": 0.15714285714285717,
          "recall": 0.1375,
          "f1_score": 0.14666666666666667,
          "date": "2025-04-06T08:48:01.122609",
          "details": {
            "precision": 0.15714285714285717,
            "recall": 0.1375,
            "f1_score": 0.14666666666666667,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "replacement": {
                "precision": 0.55,
                "recall": 0.3666666666666667,
                "f1": 0.44,
                "count": 3
              }
            },
            "correct_count": 1,
            "total_ground_truth": 8,
            "total_predicted": 7,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit type ('replacement') matches the ground truth exactly. The text content accuracy is also correct; both the ground truth and the prediction correctly show the original text 'Saone' replaced by the corrected form 'Sa\u00f4ne'. Despite differing page numbers (which are not part of evaluation criteria here), the essential intended correction (adding the accent) is fully captured and matches exactly in both predicted and ground truth edits.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              },
              {
                "is_correct": false,
                "reasoning": "While the edit type ('punctuation') matches exactly, the predicted corrected text misses the comma included in the ground truth ('poplar-trees,'). The ground truth indicates both adding a hyphen and a comma ('poplar-trees,'), but the predicted correction only captured the hyphen ('poplar-trees') without the comma. Therefore, the essential punctuation correction is only partially captured, making the prediction incorrect according to the evaluation criteria.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.9,
                "line_diff": 3
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit type correctly matches the ground truth edit type ('replacement'). The textual content of the predicted edit matches exactly the ground truth\u2014the original word 'Saone' is corrected to 'Sa\u00f4ne' in both edits. Since we are instructed explicitly to ignore differences in line numbers and pages for this evaluation, the predicted edit accurately captures the core intended editorial correction.",
                "is_correct_with_penalty": false,
                "score": 0.09999999999999998,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "replacement",
                  "original_text": "Saone",
                  "corrected_text": "Sa\u00f4ne",
                  "line_number": 1,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "Saone",
                  "corrected_text": "Sa\u00f4ne",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees,",
                  "line_number": 6,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees",
                  "line_number": 3,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "replacement",
                  "original_text": "Saone",
                  "corrected_text": "Sa\u00f4ne",
                  "line_number": 7,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "Saone",
                  "corrected_text": "Sa\u00f4ne",
                  "line_number": 4,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        },
        {
          "model_name": "or_mistral_small_3_1_24b_instruct",
          "precision": 0.2,
          "recall": 0.125,
          "f1_score": 0.15384615384615385,
          "date": "2025-04-06T08:48:58.370012",
          "details": {
            "precision": 0.2,
            "recall": 0.125,
            "f1_score": 0.15384615384615385,
            "by_type": {
              "replacement": {
                "precision": 1.0,
                "recall": 0.3333333333333333,
                "f1": 0.5,
                "count": 3
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              }
            },
            "correct_count": 1,
            "total_ground_truth": 8,
            "total_predicted": 5,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The edit type ('punctuation') matches exactly, meeting the first criterion. However, the text content accuracy is incorrect. The ground truth correction shows the addition of a comma between 'dull' and 'autumn' (changing 'dull autumn' to 'dull, autumn'), while the prediction incorrectly placed the comma between 'late' and 'dull', resulting in 'late, dull autumn night'. This misrepresents the intended editorial correction, positioning punctuation at a different location and thereby not accurately capturing the core change identified by the ground truth.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              },
              {
                "is_correct": true,
                "reasoning": "The edit type matches accurately, as both ground truth and prediction specify a 'replacement' edit. The textual correction is also accurately captured; the essential change from 'Saone' to 'Sa\u00f4ne' is correctly identified. Though the predicted edit includes additional context ('river'), this does not conflict with the core edit intention described in the ground truth. Thus, the prediction is considered correct.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              },
              {
                "is_correct": false,
                "reasoning": "The edit type matches exactly ('punctuation'), satisfying the first criterion. However, the predicted edit did not apply the intended punctuation correction ('poplar trees' to 'poplar-trees,'). The ground truth explicitly includes a hyphen and comma ('poplar-trees,'), indicating the necessity for punctuation changes. Since the prediction left the text unchanged ('poplar trees'), it fails to capture the essential content change intended by the ground truth edit.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "dull autumn",
                  "corrected_text": "dull, autumn",
                  "line_number": 1,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "late dull autumn night",
                  "corrected_text": "late, dull autumn night",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "replacement",
                  "original_text": "Saone",
                  "corrected_text": "Sa\u00f4ne",
                  "line_number": 1,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "river Saone",
                  "corrected_text": "river Sa\u00f4ne",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees,",
                  "line_number": 6,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar trees",
                  "line_number": 3,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        }
      ]
    }
  },
  {
    "model_name": "Llama 4 Maverick",
    "precision": 0.14545454545454545,
    "recall": 0.09876543209876543,
    "f1_score": 0.11764705882352941,
    "date": "2025-04-06T08:22:41.390445",
    "shots": 2,
    "config": {
      "model_id": "or_llama_4_maverick",
      "display_name": "Llama 4 Maverick",
      "shots": 2,
      "temperature": 0.0,
      "date": "2025-04-06",
      "notes": "Benchmark run with 2-shot learning"
    },
    "details": {
      "precision": 0.14545454545454545,
      "recall": 0.09876543209876543,
      "f1_score": 0.11764705882352941,
      "by_type": {
        "capitalization": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 9
        },
        "punctuation": {
          "precision": 0.16416666666666666,
          "recall": 0.07666666666666666,
          "f1": 0.09666666666666668,
          "count": 60
        },
        "replacement": {
          "precision": 0.19999999999999998,
          "recall": 0.15833333333333333,
          "f1": 0.17500000000000002,
          "count": 12
        },
        "reordering": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        },
        "insertion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        }
      },
      "correct_count": 8,
      "total_ground_truth": 81,
      "total_predicted": 55,
      "file_results": [
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.45,
          "recall": 0.18,
          "f1_score": 0.2571428571428572,
          "date": "2025-04-05T20:20:22.614100",
          "details": {
            "precision": 0.45,
            "recall": 0.18,
            "f1_score": 0.2571428571428572,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.45,
                "recall": 0.225,
                "f1": 0.3,
                "count": 4
              }
            },
            "correct_count": 1,
            "total_ground_truth": 5,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The edit type ('punctuation') matches exactly. The core textual change\u2014adding a comma after 'pockets' in 'pockets and' resulting in 'pockets, and'\u2014is also accurately captured. Differences in line number and page labeling are explicitly ignored per evaluation instructions. Thus, the predicted edit accurately reflects the ground truth intention.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "pockets and",
                  "corrected_text": "pockets, and",
                  "line_number": 17,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "pockets and",
                  "corrected_text": "pockets, and",
                  "line_number": 18,
                  "page": "page_19",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "003"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.1125,
          "recall": 0.18,
          "f1_score": 0.13846153846153847,
          "date": "2025-04-06T08:18:10.355753",
          "details": {
            "precision": 0.1125,
            "recall": 0.18,
            "f1_score": 0.13846153846153847,
            "by_type": {
              "punctuation": {
                "precision": 0.1125,
                "recall": 0.225,
                "f1": 0.15,
                "count": 4
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 5,
            "total_predicted": 8,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit type ('punctuation') matches exactly the ground truth edit type. The essential punctuation change from 'pockets and' to 'pockets, and' in the ground truth is effectively captured by the prediction ('pockets' to 'pockets,'). Although the prediction has less context (only 'pockets' rather than 'pockets and'), the core intention\u2014adding a comma after 'pockets'\u2014is accurately identified. According to evaluation criteria, additional surrounding context is allowed as long as the essential edit matches. Here, the essential punctuation edit is correctly captured, fulfilling both specified criteria.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "pockets and",
                  "corrected_text": "pockets, and",
                  "line_number": 17,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "pockets",
                  "corrected_text": "pockets,",
                  "line_number": 16,
                  "page": "page_19.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "003"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:18:38.738924",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 9,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The predicted edit correctly matches the 'punctuation' type of the ground truth. However, the text content accuracy criterion is not met. The ground truth involves adding a comma after 'yonder' (i.e., changing 'yonder to-day' to 'yonder, to-day'), while the predicted edit incorrectly modifies the text as 'to-day' to 'to day,'. This prediction alters the punctuation of 'to-day' itself and incorrectly separates the hyphenated word, failing to accurately capture the core punctuation change intended by the ground truth. Therefore, the predicted edit does not reflect the essential correction accurately and is incorrect.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "yonder to-day",
                  "corrected_text": "yonder, to-day",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "to-day",
                  "corrected_text": "to day,",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "003"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:20:41.131050",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "reordering": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:18:56.043732",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:19:09.475641",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "reordering": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.45,
          "recall": 0.45,
          "f1_score": 0.45,
          "date": "2025-04-05T20:21:14.474893",
          "details": {
            "precision": 0.45,
            "recall": 0.45,
            "f1_score": 0.45,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.9,
                "recall": 0.9,
                "f1": 0.9,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 2,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the edit type 'replacement', which exactly agrees with the ground truth's edit type. The core text change ('said' to 'asked') is accurately captured in both the prediction and ground truth. Even though the prediction provides slightly less context ('said' \u2192 'asked') compared to the ground truth ('said Clennam' \u2192 'asked Clennam'), the actual essential edit has been correctly identified. According to the guidelines, additional or fewer contextual words around the core edited text do not affect correctness as long as the main edit intention is accurately reflected. Therefore, the predicted edit satisfies all evaluation criteria for accuracy.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "replacement",
                  "original_text": "said Clennam",
                  "corrected_text": "asked Clennam",
                  "line_number": 39,
                  "page": "005.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "said",
                  "corrected_text": "asked",
                  "line_number": 40,
                  "page": "page_209",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "005"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:19:20.935032",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 1,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:19:30.495886",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.5,
          "recall": 0.125,
          "f1_score": 0.2,
          "date": "2025-04-05T20:21:49.918249",
          "details": {
            "precision": 0.5,
            "recall": 0.125,
            "f1_score": 0.2,
            "by_type": {
              "punctuation": {
                "precision": 0.5,
                "recall": 0.125,
                "f1": 0.2,
                "count": 8
              }
            },
            "correct_count": 1,
            "total_ground_truth": 8,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit has correctly captured the edit type as 'punctuation', matching the ground truth exactly. Regarding text content accuracy, the predicted edit includes an additional word ('came') after the corrected phrase; however, the essential punctuation correction ('about and' \u2192 'about, and') identified by the ground truth is clearly and accurately reflected within the predicted edit. Since the instructions clearly allow additional context and the essential core edit is correct, the prediction meets both criteria\u2014edit type accuracy and text content accuracy\u2014and is thus considered correctly captured.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "about and",
                  "corrected_text": "about, and",
                  "line_number": 13,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "about and came",
                  "corrected_text": "about, and came",
                  "line_number": 13,
                  "page": "352.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "006"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.12,
          "recall": 0.075,
          "f1_score": 0.0923076923076923,
          "date": "2025-04-06T08:20:07.610830",
          "details": {
            "precision": 0.12,
            "recall": 0.075,
            "f1_score": 0.0923076923076923,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.15,
                "recall": 0.075,
                "f1": 0.1,
                "count": 8
              }
            },
            "correct_count": 1,
            "total_ground_truth": 8,
            "total_predicted": 5,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The edit type matches exactly ('punctuation' vs. 'punctuation'), which is good. However, the text content accuracy is incorrect because the predicted edit states the original text was 'about,and' (with no space), while the ground truth explicitly shows 'about and' (with a space). The ground truth intention was to add a comma after 'about', thus changing 'about and' to 'about, and'. The predicted edit incorrectly assumes no space existed before the correction, changing 'about,and' to 'about, and'. Therefore, as the original text does not match, the prediction does not accurately reflect the essential edit described by the ground truth.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.4,
                "line_diff": 2
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the ground truth edit type ('punctuation' is exactly the same). For text content accuracy, the ground truth shows a change from 'said' to 'said,' indicating the addition of a comma. The prediction similarly made this same punctuation addition ('said' \u2192 'said,'), but included extra context ('and said' \u2192 'and said,'). This additional context ('and') does not change the fundamental correctness. The core punctuation correction to the word 'said' has been properly identified and handled by the prediction. Therefore, the predicted edit correctly captures the intention of the ground truth edit.",
                "is_correct_with_penalty": true,
                "score": 0.6,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "about and",
                  "corrected_text": "about, and",
                  "line_number": 13,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "about,and",
                  "corrected_text": "about, and",
                  "line_number": 11,
                  "page": "352.png",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "said",
                  "corrected_text": "said,",
                  "line_number": 25,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "and said",
                  "corrected_text": "and said,",
                  "line_number": 23,
                  "page": "352.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "006"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.24,
          "recall": 0.15,
          "f1_score": 0.1846153846153846,
          "date": "2025-04-06T08:20:45.434720",
          "details": {
            "precision": 0.24,
            "recall": 0.15,
            "f1_score": 0.1846153846153846,
            "by_type": {
              "punctuation": {
                "precision": 0.3,
                "recall": 0.15,
                "f1": 0.2,
                "count": 8
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 2,
            "total_ground_truth": 8,
            "total_predicted": 5,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the edit type ('punctuation') exactly as specified by the ground truth. Both the ground truth and the prediction indicate the insertion of a comma between 'about' and 'and', resulting in 'about, and'. Although the predicted original text uses 'about,and' without the space, the core intended correction\u2014adding punctuation\u2014is accurately captured by the prediction. Extra or missing spaces adjacent to punctuation are considered minor contextual differences and do not affect the correctness in capturing the core punctuation edit. Therefore, the prediction accurately matches the intention of the ground truth edit.",
                "is_correct_with_penalty": true,
                "score": 0.6,
                "line_number_penalty": 0.4,
                "line_diff": 2
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit and ground truth edit both correctly identify and apply a punctuation change (adding a comma after the word 'said'). The edit types match exactly ('punctuation'). Although the predicted version includes additional context ('and said' instead of just 'said'), the core change accurately matches precisely what the ground truth intended ('said' \u2192 'said,'). Additional context does not affect correctness as per evaluation criteria; thus, the predicted edit correctly captures the intention.",
                "is_correct_with_penalty": true,
                "score": 0.6,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "about and",
                  "corrected_text": "about, and",
                  "line_number": 13,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "about,and",
                  "corrected_text": "about, and",
                  "line_number": 11,
                  "page": "352.png",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "said",
                  "corrected_text": "said,",
                  "line_number": 25,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "and said",
                  "corrected_text": "and said,",
                  "line_number": 23,
                  "page": "352.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "006"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:22:10.518660",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:20:57.796816",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:21:17.715033",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:22:57.156550",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 3,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The edit type 'punctuation' matches exactly between prediction and ground truth. However, the text content accuracy criterion is not fully met. The ground truth specifies changing 'poplar trees' to 'poplar-trees,' (with both a hyphen and a comma). The predicted edit captures only 'poplar-trees' missing the crucial punctuation mark (comma) intended by the ground truth. Additionally, the prediction incorrectly identifies the original text as 'poplat trees' instead of 'poplar trees.' Due to these discrepancies in punctuation completeness and accurate original text representation, the prediction does not accurately capture the intention of the ground truth edit.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              },
              {
                "is_correct": false,
                "reasoning": "The predicted edit and the ground truth edit share the same edit type ('replacement'), which matches correctly. However, the essential textual correction differs. The ground truth edit modifies 'sodden' to 'soddened', indicating the correction involves changing the form of a single word. The predicted edit changes 'soddenwith' to 'sodden with', a spacing correction separating two words. Therefore, even though the edit type matches, the core textual correction indicated by the ground truth ('sodden' \u2192 'soddened') is not captured by the prediction ('soddenwith' \u2192 'sodden with'). Thus, the prediction is not considered correct.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees,",
                  "line_number": 6,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "poplat trees",
                  "corrected_text": "poplar-trees",
                  "line_number": 6,
                  "page": "page_137",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "replacement",
                  "original_text": "sodden",
                  "corrected_text": "soddened",
                  "line_number": 14,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "soddenwith",
                  "corrected_text": "sodden with",
                  "line_number": 12,
                  "page": "page_137",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.3333333333333333,
          "recall": 0.125,
          "f1_score": 0.18181818181818182,
          "date": "2025-04-06T08:21:55.157169",
          "details": {
            "precision": 0.3333333333333333,
            "recall": 0.125,
            "f1_score": 0.18181818181818182,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "replacement": {
                "precision": 0.5,
                "recall": 0.3333333333333333,
                "f1": 0.4,
                "count": 3
              }
            },
            "correct_count": 1,
            "total_ground_truth": 8,
            "total_predicted": 3,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The prediction's edit type ('replacement') correctly matches the ground truth edit type ('replacement'). Additionally, the predicted correction from 'Saone' to 'Sa\u00f4ne' matches exactly the core intention of the ground truth edit, capturing the precise textual correction (adding the accent on the 'o'). The difference in page information ('008.png' vs 'page_137') is irrelevant as per the evaluation criteria since the instruction explicitly states to ignore such metadata. Thus, both criteria (edit type and text content accuracy) are fully satisfied.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              },
              {
                "is_correct": false,
                "reasoning": "While the predicted edit correctly identifies the edit type ('punctuation'), it incorrectly captures the text content change. The ground truth edit changes 'poplar trees' to 'poplar-trees,' explicitly inserting both a hyphen and a comma. However, the predicted correction is 'poplartrees,' omitting both the hyphen and comma, thus significantly altering the intended punctuation correction. Therefore, the predicted edit does not accurately reflect the intended correction.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "replacement",
                  "original_text": "Saone",
                  "corrected_text": "Sa\u00f4ne",
                  "line_number": 1,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "Saone",
                  "corrected_text": "Sa\u00f4ne",
                  "line_number": 1,
                  "page": "page_137",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees,",
                  "line_number": 6,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplartrees",
                  "line_number": 4,
                  "page": "page_137",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        },
        {
          "model_name": "or_llama_4_maverick",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:22:41.390445",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 3,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The edit type ('punctuation') matches exactly between the ground truth and the prediction. However, while the prediction correctly captures the insertion of the hyphen ('poplar-trees'), it fails to include the additional comma that appears in the ground truth correction ('poplar-trees,'). Additionally, the predicted original text ('poplat trees') contains a typo ('poplat' instead of 'poplar'), which suggests confusion in accurately identifying the original content being corrected. Since the prediction omits the comma which is a core part of the punctuation correction and introduces an incorrect original spelling, it does not fully capture the intended ground truth edit.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              },
              {
                "is_correct": false,
                "reasoning": "The edit type matches correctly ('replacement' in both ground truth and prediction), but the text content does not accurately match the ground truth intention. The ground truth edit changes 'sodden' to 'soddened', thereby modifying the word itself by adding 'ed'. However, the predicted edit changes 'soddenwith' to 'sodden with', addressing a spacing issue instead of changing the word form. Since the core intention\u2014changing 'sodden' to 'soddened'\u2014is not captured by the predicted edit, the prediction is incorrect.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees,",
                  "line_number": 6,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "poplat trees",
                  "corrected_text": "poplar-trees",
                  "line_number": 6,
                  "page": "page_137",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "replacement",
                  "original_text": "sodden",
                  "corrected_text": "soddened",
                  "line_number": 14,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "soddenwith",
                  "corrected_text": "sodden with",
                  "line_number": 12,
                  "page": "page_137",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        }
      ]
    }
  },
  {
    "model_name": "GPT-4o",
    "precision": 0.16,
    "recall": 0.07407407407407407,
    "f1_score": 0.10126582278481013,
    "date": "2025-04-06T00:03:51.681441",
    "shots": 2,
    "config": {
      "model_id": "or_gpt_4_turbo",
      "display_name": "GPT-4o",
      "shots": 2,
      "temperature": 0.0,
      "date": "2025-04-05",
      "notes": "Benchmark run with 2-shot learning"
    },
    "details": {
      "precision": 0.16,
      "recall": 0.07407407407407407,
      "f1_score": 0.10126582278481013,
      "by_type": {
        "punctuation": {
          "precision": 0.36500000000000005,
          "recall": 0.08499999999999999,
          "f1": 0.1161111111111111,
          "count": 40
        },
        "insertion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        },
        "capitalization": {
          "precision": 0.016666666666666663,
          "recall": 0.016666666666666663,
          "f1": 0.016666666666666663,
          "count": 6
        },
        "replacement": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 8
        }
      },
      "correct_count": 4,
      "total_ground_truth": 54,
      "total_predicted": 25,
      "file_results": [
        {
          "model_name": "or_gpt_4_turbo",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T00:01:24.894591",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "or_gpt_4_turbo",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T23:49:13.927092",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 1,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "or_gpt_4_turbo",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T00:01:46.581281",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_gpt_4_turbo",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T23:49:34.518643",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_gpt_4_turbo",
          "precision": 0.04999999999999999,
          "recall": 0.04999999999999999,
          "f1_score": 0.04999999999999999,
          "date": "2025-04-06T00:02:19.248786",
          "details": {
            "precision": 0.04999999999999999,
            "recall": 0.04999999999999999,
            "f1_score": 0.04999999999999999,
            "by_type": {
              "capitalization": {
                "precision": 0.09999999999999998,
                "recall": 0.09999999999999998,
                "f1": 0.09999999999999998,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly identifies the edit type ('capitalization') exactly matching the ground truth. Considering text content accuracy, the ground truth edit is 'if he' changed to 'If he', and the predicted edit provides 'if' to 'If'. The prediction includes less context than the ground truth, but it still clearly captures the core intended correction: the capitalization of 'if' to 'If'. Since additional context is allowed (either more or fewer words around the edit), and the change accurately matches the essential edit of capitalizing 'if', the predicted edit is considered correct.",
                "is_correct_with_penalty": false,
                "score": 0.09999999999999998,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "capitalization",
                  "original_text": "if he",
                  "corrected_text": "If he",
                  "line_number": 36,
                  "page": "005.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "capitalization",
                  "original_text": "if",
                  "corrected_text": "If",
                  "line_number": 33,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "005"
        },
        {
          "model_name": "or_gpt_4_turbo",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T23:49:52.775627",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_gpt_4_turbo",
          "precision": 0.3,
          "recall": 0.075,
          "f1_score": 0.12,
          "date": "2025-04-06T00:02:52.780224",
          "details": {
            "precision": 0.3,
            "recall": 0.075,
            "f1_score": 0.12,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.6,
                "recall": 0.075,
                "f1": 0.13333333333333333,
                "count": 8
              }
            },
            "correct_count": 1,
            "total_ground_truth": 8,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly captures the core punctuation change intended by the ground truth. Both the ground truth and the prediction identify that a comma needs to be added immediately after the word 'said'. The edit type ('punctuation') precisely matches. Although the ground truth explicitly states the context as 'said when' \u2192 'said, when', and the prediction only references 'said' \u2192 'said,', this is still acceptable because the essential punctuation correction (adding a comma after 'said') is correctly captured. Differences in the provided context length or additional surrounding words are allowed as long as they correctly capture the core intended punctuation change.",
                "is_correct_with_penalty": true,
                "score": 0.6,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "said when",
                  "corrected_text": "said, when",
                  "line_number": 23,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "said",
                  "corrected_text": "said,",
                  "line_number": 25,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "006"
        },
        {
          "model_name": "or_gpt_4_turbo",
          "precision": 0.5,
          "recall": 0.125,
          "f1_score": 0.2,
          "date": "2025-04-05T23:50:26.807652",
          "details": {
            "precision": 0.5,
            "recall": 0.125,
            "f1_score": 0.2,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 1.0,
                "recall": 0.125,
                "f1": 0.2222222222222222,
                "count": 8
              }
            },
            "correct_count": 1,
            "total_ground_truth": 8,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the ground truth edit in terms of both criteria evaluated. Specifically, (1) 'Edit Type Accuracy' matches exactly, as both are labeled 'punctuation'. (2) In terms of 'Text Content Accuracy', the ground truth edit ('said when' becoming 'said, when') includes 'said' followed by inserting a comma. The predicted edit ('said' becoming 'said,') clearly captures the exact intended punctuation adjustment. Even though the predicted edit omits the trailing word 'when', the critical punctuation change from 'said' to 'said,' aligns exactly with the core intention of the ground truth correction. Therefore, the predicted edit accurately captures the essential intended edit.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "said when",
                  "corrected_text": "said, when",
                  "line_number": 23,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "said",
                  "corrected_text": "said,",
                  "line_number": 23,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "006"
        },
        {
          "model_name": "or_gpt_4_turbo",
          "precision": 0.9,
          "recall": 0.9,
          "f1_score": 0.9,
          "date": "2025-04-06T00:03:26.893097",
          "details": {
            "precision": 0.9,
            "recall": 0.9,
            "f1_score": 0.9,
            "by_type": {
              "punctuation": {
                "precision": 0.9,
                "recall": 0.9,
                "f1": 0.9,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly identifies the edit type as 'punctuation', which matches exactly the ground truth. Furthermore, the text content matches precisely the ground truth intention: both the original text ('Sun and Shadow') and the corrected text ('Sun and Shadow.') are accurately captured in the prediction. Although page numbers differ, the instruction explicitly states to ignore line numbers (and implicitly other positional information), so this difference does not affect the correctness. Thus, the prediction correctly captures the core editorial intention.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "007.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "007"
        },
        {
          "model_name": "or_gpt_4_turbo",
          "precision": 0.9,
          "recall": 0.9,
          "f1_score": 0.9,
          "date": "2025-04-05T23:50:58.267387",
          "details": {
            "precision": 0.9,
            "recall": 0.9,
            "f1_score": 0.9,
            "by_type": {
              "punctuation": {
                "precision": 0.9,
                "recall": 0.9,
                "f1": 0.9,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the ground truth edit. Both the predicted and ground truth edit specify the exact same type ('punctuation'), and both accurately indicate the same core textual modification: adding a period at the end of 'Sun and Shadow'. The difference in line numbers and page numbers is explicitly ignored per evaluation instructions. Therefore, the edit type and text content criteria are fully satisfied.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "007.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "007"
        },
        {
          "model_name": "or_gpt_4_turbo",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T00:03:51.681441",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 4,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The predicted edit matches the ground truth edit type ('punctuation'), but fails to fully capture the text content accurately. The ground truth correction explicitly changes 'poplar trees' to 'poplar-trees,' (adding both a hyphen and comma). However, the predicted edit only adds a hyphen ('poplar-trees') and misses the intended comma. Thus, the prediction does not fully represent the punctuation change indicated by the ground truth.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees,",
                  "line_number": 6,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees",
                  "line_number": 5,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        },
        {
          "model_name": "or_gpt_4_turbo",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T23:51:32.956365",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 4,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "Although the edit type (punctuation) matches exactly, the text content accuracy is incomplete. The ground truth indicates that \"poplar trees\" should be corrected to \"poplar-trees,\" (adding both a hyphen and a comma). The predicted edit includes only the hyphen (\"poplar-trees\") and misses the comma at the end, which is essential for fully capturing the ground truth's intended punctuation correction. Thus, the predicted edit only partially captures the intended change, and therefore is incorrect according to the evaluation criteria.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees,",
                  "line_number": 6,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees",
                  "line_number": 5,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        }
      ]
    }
  },
  {
    "model_name": "Qwen VL Max",
    "precision": 0.12,
    "recall": 0.07407407407407407,
    "f1_score": 0.0916030534351145,
    "date": "2025-04-06T08:41:01.057854",
    "shots": 2,
    "config": {
      "model_id": "or_qwen_vl_max",
      "display_name": "Qwen VL Max",
      "shots": 2,
      "temperature": 0.0,
      "date": "2025-04-06",
      "notes": "Benchmark run with 2-shot learning"
    },
    "details": {
      "precision": 0.12,
      "recall": 0.07407407407407407,
      "f1_score": 0.0916030534351145,
      "by_type": {
        "capitalization": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 9
        },
        "punctuation": {
          "precision": 0.145,
          "recall": 0.075,
          "f1": 0.08277777777777778,
          "count": 60
        },
        "insertion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        },
        "replacement": {
          "precision": 0.024999999999999998,
          "recall": 0.049999999999999996,
          "f1": 0.03333333333333333,
          "count": 12
        },
        "deletion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        }
      },
      "correct_count": 6,
      "total_ground_truth": 81,
      "total_predicted": 50,
      "file_results": [
        {
          "model_name": "qwen/qwen-vl-max",
          "precision": 0.225,
          "recall": 0.18,
          "f1_score": 0.19999999999999998,
          "date": "2025-04-05T02:49:09.914762",
          "details": {
            "precision": 0.225,
            "recall": 0.18,
            "f1_score": 0.19999999999999998,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.225,
                "recall": 0.225,
                "f1": 0.225,
                "count": 4
              }
            },
            "correct_count": 1,
            "total_ground_truth": 5,
            "total_predicted": 4,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit has the exact same 'punctuation' edit as the ground truth, adding a comma after the word 'yonder.' Although the predicted edit includes an extra word ('over') before the context ('yonder to-day'), this additional context does not alter or obscure the essential punctuation edit made ('yonder' \u2192 'yonder,'). Thus, edit type and the core textual change accurately match the ground truth's intention.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "yonder to-day",
                  "corrected_text": "yonder, to-day",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "over yonder to-day",
                  "corrected_text": "over yonder, to-day",
                  "line_number": 2,
                  "page": "19",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "003"
        },
        {
          "model_name": "or_qwen_vl_max",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:36:10.863166",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 4,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "or_qwen_vl_max",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:36:36.907969",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 3,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The predicted edit correctly identifies the edit type as 'punctuation', matching the ground truth. However, the predicted correction incorrectly applies punctuation after 'to-day' alone ('to-day,'), whereas the ground truth indicates the insertion of punctuation between 'yonder' and 'to-day' ('yonder, to-day'). Thus, the essential edit location and punctuation placement differ, failing the text content accuracy criterion.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "yonder to-day",
                  "corrected_text": "yonder, to-day",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "to-day",
                  "corrected_text": "to-day,",
                  "line_number": 1,
                  "page": "19",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "003"
        },
        {
          "model_name": "qwen/qwen-vl-max",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T02:49:20.522971",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 4,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_qwen_vl_max",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:36:55.399404",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_qwen_vl_max",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:37:12.062961",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "qwen/qwen-vl-max",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T02:49:35.059127",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_qwen_vl_max",
          "precision": 0.19999999999999998,
          "recall": 0.3,
          "f1_score": 0.23999999999999996,
          "date": "2025-04-06T08:37:34.258177",
          "details": {
            "precision": 0.19999999999999998,
            "recall": 0.3,
            "f1_score": 0.23999999999999996,
            "by_type": {
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.3,
                "recall": 0.6,
                "f1": 0.4,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 2,
            "total_predicted": 3,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly captures the intended change from the ground truth. Both edits clearly reflect the edit type as a 'replacement', matching exactly. Regarding text content accuracy, although the predicted edit contains additional punctuation after 'Clennam', this reflects additional context and does not alter the core intended correction ('said'\u2192'asked'). Thus, the essential edit intention is accurately captured.",
                "is_correct_with_penalty": true,
                "score": 0.6,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "replacement",
                  "original_text": "said Clennam",
                  "corrected_text": "asked Clennam",
                  "line_number": 39,
                  "page": "005.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "said Clennam.",
                  "corrected_text": "asked Clennam.",
                  "line_number": 41,
                  "page": "209",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "005"
        },
        {
          "model_name": "or_qwen_vl_max",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:37:47.034381",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "qwen/qwen-vl-max",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T02:50:14.218090",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 3,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The edit type ('punctuation') matches correctly. However, the predicted edit places the punctuation after 'and' (changing 'and' to 'and,'), whereas the ground truth edit intends punctuation between the words 'about and', resulting in 'about, and'. Therefore, the essential punctuation insertion is not properly captured\u2014the prediction addresses the wrong punctuation placement and fails to reflect the intended correction accurately.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.9,
                "line_diff": 3
              },
              {
                "is_correct": false,
                "reasoning": "Edit type accuracy is correct ('punctuation' matches 'punctuation'). However, the text content accuracy is incorrect. The ground truth explicitly indicates that the original text was 'said when' changed to 'said, when', clearly placing the comma between 'said' and 'when'. The prediction only shows an isolated change ('said' to 'said,') without including the context word 'when', making it unclear if this fully matches the ground truth intent. Although the essential punctuation inserted matches (comma after 'said'), the omission of the word 'when' means it does not precisely reflect the intended edit specified by the ground truth. Therefore, the predicted edit is incorrect.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "about and",
                  "corrected_text": "about, and",
                  "line_number": 13,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "and",
                  "corrected_text": "and,",
                  "line_number": 10,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "said when",
                  "corrected_text": "said, when",
                  "line_number": 23,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "said",
                  "corrected_text": "said,",
                  "line_number": 20,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "006"
        },
        {
          "model_name": "or_qwen_vl_max",
          "precision": 0.19999999999999998,
          "recall": 0.075,
          "f1_score": 0.10909090909090909,
          "date": "2025-04-06T08:38:18.136476",
          "details": {
            "precision": 0.19999999999999998,
            "recall": 0.075,
            "f1_score": 0.10909090909090909,
            "by_type": {
              "punctuation": {
                "precision": 0.6,
                "recall": 0.075,
                "f1": 0.13333333333333333,
                "count": 8
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 1,
            "total_ground_truth": 8,
            "total_predicted": 3,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The prediction correctly identifies the edit type ('punctuation'), matching exactly with the ground truth. Considering the text content, the ground truth indicated a change from 'said when' to 'said, when', essentially inserting a comma after 'said'. The prediction shows the essential change ('said' \u2192 'said,') clearly, without contradicting or misunderstanding the intended edit. Even though the prediction provides less context by not explicitly including 'when', it accurately captures the core punctuation edit being made (adding a comma after 'said'). Hence, both edit type and text content accuracy criteria are satisfied.",
                "is_correct_with_penalty": true,
                "score": 0.6,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "said when",
                  "corrected_text": "said, when",
                  "line_number": 23,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "said",
                  "corrected_text": "said,",
                  "line_number": 25,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "006"
        },
        {
          "model_name": "or_qwen_vl_max",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:39:12.673538",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 4,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The predicted edit correctly identifies the edit type as 'punctuation', matching the ground truth's type. However, the ground truth explicitly shows an insertion of a comma between 'materials' and 'costing' ('materials costing' \u2192 'materials, costing'). The prediction does not reflect this change and instead suggests no actual modification ('costing' \u2192 'costing'), ignoring the essential comma insertion. Thus, the prediction fails to capture the core textual correction identified by the ground truth.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.4,
                "line_diff": 2
              },
              {
                "is_correct": false,
                "reasoning": "The edit type ('punctuation') matches correctly. However, the text content accuracy does not match the ground truth. The ground truth edit is 'about and' \u2192 'about, and' (adding a comma after 'about'), whereas the prediction given is 'and' \u2192 'and,' (adding a comma directly after 'and'). This difference significantly changes the intended punctuation location, and thus fails to capture the core change accurately. Therefore, although the edit type is correct, the essential edit ('text content accuracy') is incorrect.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.9,
                "line_diff": 3
              },
              {
                "is_correct": false,
                "reasoning": "While the edit type ('punctuation') matches correctly, the prediction does not accurately capture the intended textual change. The ground truth edit specifically corrects 'said when' to 'said, when', indicating the insertion of a comma between 'said' and 'when'. The predicted edit correctly identifies the punctuation change but incorrectly limits the original and corrected text to 'said' \u2192 'said,', omitting the crucial context ('when'). Because the core context word 'when' is essential to capturing the exact intention of the original annotation, its absence in the predicted text makes this prediction inaccurate.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "materials costing",
                  "corrected_text": "materials, costing",
                  "line_number": 3,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "costing",
                  "corrected_text": "costing",
                  "line_number": 1,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "about and",
                  "corrected_text": "about, and",
                  "line_number": 13,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "and",
                  "corrected_text": "and,",
                  "line_number": 10,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "said when",
                  "corrected_text": "said, when",
                  "line_number": 23,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "said",
                  "corrected_text": "said,",
                  "line_number": 20,
                  "page": "352",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "006"
        },
        {
          "model_name": "qwen/qwen-vl-max",
          "precision": 1.0,
          "recall": 1.0,
          "f1_score": 1.0,
          "date": "2025-04-05T02:50:42.623550",
          "details": {
            "precision": 1.0,
            "recall": 1.0,
            "f1_score": 1.0,
            "by_type": {
              "punctuation": {
                "precision": 1.0,
                "recall": 1.0,
                "f1": 1.0,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The prediction correctly identifies the edit type as 'punctuation', exactly matching the ground truth. It also correctly captures the essential content, identifying clearly the change from 'Sun and Shadow' to 'Sun and Shadow.', matching the core punctuation correction indicated by the ground truth. Although the 'page' identifier differs ('Chapter 1' rather than '007.png'), this is irrelevant since the line number and page context are explicitly ignored per instructions. Therefore, the prediction fully matches the edit type and captures the core textual modification accurately.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "007.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "Chapter 1",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "007"
        },
        {
          "model_name": "or_qwen_vl_max",
          "precision": 1.0,
          "recall": 1.0,
          "f1_score": 1.0,
          "date": "2025-04-06T08:39:46.282575",
          "details": {
            "precision": 1.0,
            "recall": 1.0,
            "f1_score": 1.0,
            "by_type": {
              "punctuation": {
                "precision": 1.0,
                "recall": 1.0,
                "f1": 1.0,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The edit type in both ground truth ('punctuation') and prediction ('punctuation') matches exactly. The textual correction from 'Sun and Shadow' to 'Sun and Shadow.' also matches precisely, accurately capturing the core punctuation change identified in the ground truth. Although there is a difference in the 'page' indication ('007.png' vs. 'Chapter 1'), the instructions explicitly state to ignore line numbers and implicitly make no mention that page annotations should affect evaluation. Since only edit type and text content accuracy are relevant here, the prediction meets both of these criteria perfectly.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "007.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "Chapter 1",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "007"
        },
        {
          "model_name": "or_qwen_vl_max",
          "precision": 1.0,
          "recall": 1.0,
          "f1_score": 1.0,
          "date": "2025-04-06T08:40:17.996003",
          "details": {
            "precision": 1.0,
            "recall": 1.0,
            "f1_score": 1.0,
            "by_type": {
              "punctuation": {
                "precision": 1.0,
                "recall": 1.0,
                "f1": 1.0,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the edit type as 'punctuation', identical to the ground truth edit. The original text ('Sun and Shadow') and the corrected text ('Sun and Shadow.') also precisely match the ground truth intention to add a period at the end. Although the 'page' identifier in the prediction ('Chapter I') differs from the ground truth ('007.png'), this is considered ancillary information and does not impact the criteria for evaluating edit type accuracy and text content accuracy. Therefore, the predicted edit correctly captures the intention of the ground truth edit.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "007.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "Chapter I",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "007"
        },
        {
          "model_name": "qwen/qwen-vl-max",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T02:50:56.659115",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        },
        {
          "model_name": "or_qwen_vl_max",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:40:39.376007",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        },
        {
          "model_name": "or_qwen_vl_max",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:41:01.057854",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        }
      ]
    }
  },
  {
    "model_name": "GPT-4.5 Preview (OpenRouter)",
    "precision": 0.05454545454545454,
    "recall": 0.05555555555555555,
    "f1_score": 0.055045871559633024,
    "date": "2025-04-05T22:44:47.418878",
    "shots": 2,
    "config": {
      "model_id": "or_gpt_4_5_preview",
      "display_name": "GPT-4.5 Preview (OpenRouter)",
      "shots": 2,
      "temperature": 0.0,
      "date": "2025-04-05",
      "notes": "Benchmark run with 2-shot learning"
    },
    "details": {
      "precision": 0.05454545454545454,
      "recall": 0.05555555555555555,
      "f1_score": 0.055045871559633024,
      "by_type": {
        "punctuation": {
          "precision": 0.07375000000000001,
          "recall": 0.0625,
          "f1": 0.06749999999999999,
          "count": 40
        },
        "capitalization": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 6
        },
        "replacement": {
          "precision": 0.03749999999999999,
          "recall": 0.012499999999999997,
          "f1": 0.018749999999999996,
          "count": 8
        },
        "insertion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        },
        "deletion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        }
      },
      "correct_count": 3,
      "total_ground_truth": 54,
      "total_predicted": 55,
      "file_results": [
        {
          "model_name": "or_gpt_4_5_preview",
          "precision": 0.024999999999999994,
          "recall": 0.019999999999999997,
          "f1_score": 0.022222222222222216,
          "date": "2025-04-05T22:07:55.285399",
          "details": {
            "precision": 0.024999999999999994,
            "recall": 0.019999999999999997,
            "f1_score": 0.022222222222222216,
            "by_type": {
              "punctuation": {
                "precision": 0.024999999999999994,
                "recall": 0.024999999999999994,
                "f1": 0.024999999999999994,
                "count": 4
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 4,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly identifies 'punctuation' as the edit type, matching the ground truth exactly. Moreover, it accurately captures the essential punctuation change from 'pockets and' to 'pockets, and', which aligns precisely with the ground truth edit. The slight difference in line numbers is explicitly ignored for this evaluation, as instructed. Since both the edit type and the text content match the intention of the ground truth edit exactly, the predicted edit is correct.",
                "is_correct_with_penalty": false,
                "score": 0.09999999999999998,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "pockets and",
                  "corrected_text": "pockets, and",
                  "line_number": 17,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "pockets and",
                  "corrected_text": "pockets, and",
                  "line_number": 20,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "003"
        },
        {
          "model_name": "or_gpt_4_5_preview",
          "precision": 0.15,
          "recall": 0.12,
          "f1_score": 0.1333333333333333,
          "date": "2025-04-05T22:42:17.869601",
          "details": {
            "precision": 0.15,
            "recall": 0.12,
            "f1_score": 0.1333333333333333,
            "by_type": {
              "punctuation": {
                "precision": 0.15,
                "recall": 0.15,
                "f1": 0.15,
                "count": 4
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 5,
            "total_predicted": 4,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit accurately matches the ground truth in both edit type and text content. The edit type in both cases is \"punctuation\" and the textual change from \"pockets and\" to \"pockets, and\" is identical. Line numbers were explicitly instructed to be ignored, hence this mismatch does not influence evaluation. Therefore, the prediction correctly captures the intended edit.",
                "is_correct_with_penalty": true,
                "score": 0.6,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "pockets and",
                  "corrected_text": "pockets, and",
                  "line_number": 17,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "pockets and",
                  "corrected_text": "pockets, and",
                  "line_number": 19,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "003"
        },
        {
          "model_name": "or_gpt_4_5_preview",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T22:08:45.295160",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 3,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The edit type matches correctly ('capitalization'); however, the text content does not accurately reflect the intended edit in the ground truth. The ground truth edit involves capitalizing the first word ('why') in the phrase 'why did he dine', changing it to 'Why did he dine'. In contrast, the predicted edit focuses only on capitalizing the word 'he' (to 'He') without addressing the correct portion ('why') identified by the ground truth. Therefore, despite the matching edit type, the core textual change is incorrect.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.1,
                "line_diff": 1
              },
              {
                "is_correct": false,
                "reasoning": "Though the edit types match ('punctuation'), the text content accuracy is incorrect. The ground truth edit is changing an exclamation mark ('!') to a question mark ('?'), thus altering 'cold stones!' to 'cold stones?'. However, the prediction instead adds a quotation mark after the exclamation mark, suggesting 'stones!' \u2192 'stones!\"'. This does not reflect the intended punctuation change captured in the ground truth. Because the core punctuation change intended in the ground truth (from '!' to '?') was not captured, the predicted edit fails the accuracy criterion.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "capitalization",
                  "original_text": "why did he dine",
                  "corrected_text": "Why did he dine",
                  "line_number": 28,
                  "page": "004.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "capitalization",
                  "original_text": "he",
                  "corrected_text": "He",
                  "line_number": 27,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "cold stones!",
                  "corrected_text": "cold stones?",
                  "line_number": 29,
                  "page": "004.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "stones!",
                  "corrected_text": "stones!\"",
                  "line_number": 28,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "004"
        },
        {
          "model_name": "or_gpt_4_5_preview",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T22:42:55.880873",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 3,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "Although the predicted edit correctly identifies the edit type ('capitalization'), the text content does not accurately match the ground truth. The ground truth indicates the capitalization of the first word 'why' to 'Why', whereas the predicted edit addresses 'he' becoming 'He'. This difference means the predicted edit does not capture the core intended correction described by the ground truth.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.9,
                "line_diff": 3
              },
              {
                "is_correct": false,
                "reasoning": "The edit type ('punctuation') matches exactly, satisfying criterion 1. However, for text content accuracy, despite the matched added punctuation '?', the predicted original text lacks the key punctuation ('!') present in the ground truth. The ground truth specifically indicates changing 'cold stones!' into 'cold stones?', thus the essence of correction is from exclamation mark to question mark. The predicted edit mistakenly assumes adding a question mark to the plain text 'stones', missing the original exclamation mark. As a result, the predicted edit does not accurately capture the core content change indicated by the ground truth.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "capitalization",
                  "original_text": "why did he dine",
                  "corrected_text": "Why did he dine",
                  "line_number": 28,
                  "page": "004.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "capitalization",
                  "original_text": "he",
                  "corrected_text": "He",
                  "line_number": 25,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "cold stones!",
                  "corrected_text": "cold stones?",
                  "line_number": 29,
                  "page": "004.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "stones",
                  "corrected_text": "stones?",
                  "line_number": 26,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "004"
        },
        {
          "model_name": "or_gpt_4_5_preview",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T22:09:09.209803",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_gpt_4_5_preview",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T22:43:30.097536",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 3,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The predicted edit type ('capitalization') matches exactly the ground truth edit type ('capitalization'). However, the core textual content does not match. The ground truth involves capitalizing the initial word 'if' to 'If', changing 'if he' to 'If he'. The prediction instead only capitalizes the word 'he' to 'He', resulting in 'he' to 'He', and misses the crucial intended change ('if' becoming 'If'). Therefore, despite the correct edit type, the essential textual content is not accurately captured, making the overall prediction incorrect.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "capitalization",
                  "original_text": "if he",
                  "corrected_text": "If he",
                  "line_number": 36,
                  "page": "005.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "capitalization",
                  "original_text": "he",
                  "corrected_text": "He",
                  "line_number": 33,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "005"
        },
        {
          "model_name": "or_gpt_4_5_preview",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T22:09:31.038728",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 9,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "006"
        },
        {
          "model_name": "or_gpt_4_5_preview",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T22:43:50.221273",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 9,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "006"
        },
        {
          "model_name": "or_gpt_4_5_preview",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T22:10:05.357484",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "Although the edit type ('punctuation') matches exactly in the prediction and ground truth, the text content accuracy criterion is not fully met. The ground truth correction intends to change 'Sun and Shadow' to 'Sun and Shadow.', indicating the original text had no punctuation. However, the prediction incorrectly identifies the original text as 'Sun and Shadow,', assuming a comma was originally present. Due to this mismatch in original text context, the prediction fails to capture the correct initial state of the ground truth's intended edit. Therefore, the predicted edit does not accurately reflect the core change as specified by the ground truth.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "007.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow,",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "007"
        },
        {
          "model_name": "or_gpt_4_5_preview",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T22:44:09.425095",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_gpt_4_5_preview",
          "precision": 0.2571428571428572,
          "recall": 0.225,
          "f1_score": 0.24,
          "date": "2025-04-05T22:11:00.452147",
          "details": {
            "precision": 0.2571428571428572,
            "recall": 0.225,
            "f1_score": 0.24,
            "by_type": {
              "punctuation": {
                "precision": 0.45,
                "recall": 0.36,
                "f1": 0.39999999999999997,
                "count": 5
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 2,
            "total_ground_truth": 8,
            "total_predicted": 7,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The edit type ('punctuation') matches exactly. However, the text content accuracy criteria is not fully satisfied because the predicted edit ('poplar-trees') misses the comma indicated in the ground truth ('poplar-trees,'). This comma is part of the core punctuation correction intended by the ground truth, and omitting it means the essential edit has not been accurately captured.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the ground truth edit in both criteria. First, the edit type ('punctuation') exactly matches the ground truth. Second, the text content accurately reflects the essential punctuation change identified by the ground truth ('man slowly' changed to 'man, slowly'), capturing precisely the intended correction. Although the predicted edit references a different line number and page, these aspects were explicitly instructed to be ignored in the evaluation. Hence, the prediction is correct.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit matches exactly the ground truth edit in terms of edit type and text content. Both the ground truth and prediction identify the edit correctly as a punctuation edit, specifically adding a comma after 'Chalons'. The original and corrected text ('Chalons was' \u2192 'Chalons, was') in the prediction precisely matches the intention and actual change identified in the ground truth. Line numbers were explicitly ignored per instructions.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees,",
                  "line_number": 6,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees",
                  "line_number": 6,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "man slowly",
                  "corrected_text": "man, slowly",
                  "line_number": 9,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "man slowly",
                  "corrected_text": "man, slowly",
                  "line_number": 8,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "Chalons was",
                  "corrected_text": "Chalons, was",
                  "line_number": 9,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Chalons was",
                  "corrected_text": "Chalons, was",
                  "line_number": 8,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        },
        {
          "model_name": "or_gpt_4_5_preview",
          "precision": 0.014285714285714282,
          "recall": 0.012499999999999997,
          "f1_score": 0.013333333333333329,
          "date": "2025-04-05T22:44:47.418878",
          "details": {
            "precision": 0.014285714285714282,
            "recall": 0.012499999999999997,
            "f1_score": 0.013333333333333329,
            "by_type": {
              "replacement": {
                "precision": 0.09999999999999998,
                "recall": 0.033333333333333326,
                "f1": 0.04999999999999999,
                "count": 3
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 7,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "Although the predicted edit correctly matches the edit type \"punctuation\" and partially captures the suggested change by adding the hyphen (\"poplar trees\" \u2192 \"poplar-trees\"), it does not include the comma that is part of the ground truth corrected text (\"poplar-trees,\"). Because the comma was explicitly included in the original editorial correction, omitting it means the core punctuation change was not fully captured. Therefore, the prediction does not meet the text content accuracy criterion, making this prediction incorrect.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.1,
                "line_diff": 1
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the edit type ('replacement') exactly as specified in the ground truth. The core textual change from 'sodden' to 'soddened' is accurately captured in the prediction. Although the prediction includes additional context ('with'), the essential edit\u2014the transformation of 'sodden' to 'soddened'\u2014is precisely represented. According to the guidelines provided, additional context is permissible as long as the essential edit itself is accurately captured, which it is in this case. Thus, the prediction meets all the requirements and is correct.",
                "is_correct_with_penalty": false,
                "score": 0.09999999999999998,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees,",
                  "line_number": 6,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees",
                  "line_number": 5,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "replacement",
                  "original_text": "sodden",
                  "corrected_text": "soddened",
                  "line_number": 14,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "sodden with",
                  "corrected_text": "soddened with",
                  "line_number": 11,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        }
      ]
    }
  },
  {
    "model_name": "Gemma 3 27B",
    "precision": 0.047619047619047616,
    "recall": 0.041666666666666664,
    "f1_score": 0.044444444444444446,
    "date": "2025-04-05T03:07:12.806495",
    "shots": 2,
    "config": {
      "model_id": "google/gemma-3-27b-it",
      "display_name": "Gemma 3 27B",
      "shots": 2,
      "temperature": 0.0,
      "date": "2025-04-05",
      "notes": "Benchmark run with 2-shot learning"
    },
    "details": {
      "precision": 0.047619047619047616,
      "recall": 0.041666666666666664,
      "f1_score": 0.044444444444444446,
      "by_type": {
        "punctuation": {
          "precision": 0.044444444444444446,
          "recall": 0.05555555555555555,
          "f1": 0.04938271604938272,
          "count": 18
        },
        "capitalization": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 2
        },
        "insertion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        },
        "replacement": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 4
        },
        "deletion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        }
      },
      "correct_count": 1,
      "total_ground_truth": 24,
      "total_predicted": 21,
      "file_results": [
        {
          "model_name": "google/gemma-3-27b-it",
          "precision": 0.2,
          "recall": 0.2,
          "f1_score": 0.20000000000000004,
          "date": "2025-04-05T03:05:43.365632",
          "details": {
            "precision": 0.2,
            "recall": 0.2,
            "f1_score": 0.20000000000000004,
            "by_type": {
              "punctuation": {
                "precision": 0.2,
                "recall": 0.25,
                "f1": 0.22222222222222224,
                "count": 4
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 5,
            "total_predicted": 5,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "While the predicted edit shares the same edit type ('punctuation') with the ground truth, it does not capture the specific punctuation correction the ground truth identifies. The ground truth edit specifically corrects 'howling over' to 'howling, over', adding only a comma after 'howling'. However, the predicted edit suggests a completely different punctuation modification, inserting commas around 'to-day', adding '&c;', and introducing a question mark at the end, none of which match the ground truth change. Thus, the essential original intention ('howling over' \u2192 'howling, over') is not accurately captured by the prediction.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.1,
                "line_diff": 1
              },
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the ground truth's edit type of 'punctuation'. Additionally, the essential correction ('pockets and' \u2192 'pockets, and') is accurately captured without omission or alteration. Therefore, both edit type accuracy and text content accuracy align with the ground truth's intention.",
                "is_correct_with_penalty": true,
                "score": 1.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "howling over",
                  "corrected_text": "howling, over",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "o more of yesterday's howling over yonder to-day is there",
                  "corrected_text": "o more of yesterday's howling over yonder, to-day, &c; is there?",
                  "line_number": 2,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "pockets and",
                  "corrected_text": "pockets, and",
                  "line_number": 17,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "pockets and",
                  "corrected_text": "pockets, and",
                  "line_number": 17,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "003"
        },
        {
          "model_name": "google/gemma-3-27b-it",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T03:06:15.536241",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 7,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "google/gemma-3-27b-it",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T03:06:41.773832",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 3,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "Although the predicted edit correctly identifies the edit type as 'punctuation', it does not correctly capture the original and intended corrected text. The ground truth explicitly shows that the original text 'said' is changed to 'said,' indicating that the comma directly follows the word. However, the predicted correction incorrectly leaves the original text as empty ('') and describes only '(comma)' as the corrected text. This does not accurately represent the insertion's context or the specific word ('said') to which the punctuation pertains. Therefore, despite matching in edit type, the predicted edit fails to accurately represent the core punctuation change as intended in the ground truth edit.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "said",
                  "corrected_text": "said,",
                  "line_number": 25,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "",
                  "corrected_text": "(comma)",
                  "line_number": 27,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "006"
        },
        {
          "model_name": "google/gemma-3-27b-it",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T03:06:58.399054",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "google/gemma-3-27b-it",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T03:07:12.806495",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        }
      ]
    }
  },
  {
    "model_name": "GPT-4o-mini",
    "precision": 0.043478260869565216,
    "recall": 0.037037037037037035,
    "f1_score": 0.039999999999999994,
    "date": "2025-04-05T23:48:28.521617",
    "shots": 2,
    "config": {
      "model_id": "or_gpt_4o_mini",
      "display_name": "GPT-4o-mini",
      "shots": 2,
      "temperature": 0.0,
      "date": "2025-04-05",
      "notes": "Benchmark run with 2-shot learning"
    },
    "details": {
      "precision": 0.043478260869565216,
      "recall": 0.037037037037037035,
      "f1_score": 0.039999999999999994,
      "by_type": {
        "punctuation": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 20
        },
        "capitalization": {
          "precision": 0.09999999999999999,
          "recall": 0.19999999999999998,
          "f1": 0.13333333333333333,
          "count": 3
        },
        "replacement": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 4
        },
        "deletion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        },
        "insertion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        }
      },
      "correct_count": 1,
      "total_ground_truth": 27,
      "total_predicted": 23,
      "file_results": [
        {
          "model_name": "or_gpt_4o_mini",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T23:45:28.538289",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 9,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "or_gpt_4o_mini",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T23:46:04.766022",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 5,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The predicted edit correctly identified the edit type as 'punctuation', matching the ground truth. However, although the predicted correction from 'the cold stones' to 'the cold stones?' is very similar, the ground truth correction specifically addresses changing the punctuation from 'cold stones!' (with an exclamation mark) to 'cold stones?' (with a question mark). The predicted edit does not correctly identify the original punctuation mark ('!'). By missing this crucial detail, it fails to accurately capture the core edit intended in the ground truth.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "cold stones!",
                  "corrected_text": "cold stones?",
                  "line_number": 29,
                  "page": "004.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "the cold stones",
                  "corrected_text": "the cold stones?",
                  "line_number": 30,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "004"
        },
        {
          "model_name": "or_gpt_4o_mini",
          "precision": 0.3,
          "recall": 0.3,
          "f1_score": 0.3,
          "date": "2025-04-05T23:46:45.600903",
          "details": {
            "precision": 0.3,
            "recall": 0.3,
            "f1_score": 0.3,
            "by_type": {
              "capitalization": {
                "precision": 0.3,
                "recall": 0.6,
                "f1": 0.4,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 1,
            "total_ground_truth": 2,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit accurately identifies the type of edit as 'capitalization', matching exactly the ground truth edit type. Regarding text content accuracy, the core intended edit from the ground truth ('if he' to 'If he') correctly involves capitalizing the first word ('if'). The predicted edit also captures this essential edit ('if' to 'If'). While the predicted version omits the second word ('he'), it still correctly identifies the essential capitalization correction ('if' to 'If'), which is the important part of the edit. Thus, the type and core change are correctly captured by the prediction.",
                "is_correct_with_penalty": true,
                "score": 0.6,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "capitalization",
                  "original_text": "if he",
                  "corrected_text": "If he",
                  "line_number": 36,
                  "page": "005.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "capitalization",
                  "original_text": "if",
                  "corrected_text": "If",
                  "line_number": 34,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "005"
        },
        {
          "model_name": "or_gpt_4o_mini",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T23:47:10.570897",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "006"
        },
        {
          "model_name": "or_gpt_4o_mini",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T23:47:50.352070",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "Although the predicted edit correctly matches the edit type ('punctuation') and correctly identifies the intended punctuation change ('Sun and Shadow' to 'Sun and Shadow.'), it inaccurately represents the original text by showing it as 'Sun and Shadow,' (with a comma), whereas the ground truth original text is 'Sun and Shadow' (no comma). This discrepancy means it fails to accurately capture the exact punctuation correction intended in the ground truth. Thus, the prediction does not meet the strict accuracy criteria required by the evaluation guidelines.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 0,
                  "page": "007.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "Sun and Shadow,",
                  "corrected_text": "Sun and Shadow.",
                  "line_number": 1,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "007"
        },
        {
          "model_name": "or_gpt_4o_mini",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T23:48:28.521617",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 3,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "While both edits involve the same two words ('sodden' and 'soddened'), the predicted edit incorrectly reverses the direction of the intended change. The ground truth edit is changing 'sodden' into 'soddened', but the predicted edit instead suggests changing 'soddened' back into 'sodden'. Therefore, although the edit type ('replacement') matches correctly, the predicted text content direction is opposite and does not match the essential intent of the ground truth edit.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "replacement",
                  "original_text": "sodden",
                  "corrected_text": "soddened",
                  "line_number": 14,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "soddened",
                  "corrected_text": "sodden",
                  "line_number": 12,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        }
      ]
    }
  },
  {
    "model_name": "Qwen2.5 VL 72B Instruct",
    "precision": 0.05128205128205128,
    "recall": 0.029411764705882353,
    "f1_score": 0.037383177570093455,
    "date": "2025-04-06T08:38:23.817647",
    "shots": 2,
    "config": {
      "model_id": "or_qwen2_5_vl_72b_instruct",
      "display_name": "Qwen2.5 VL 72B Instruct",
      "shots": 2,
      "temperature": 0.0,
      "date": "2025-04-06",
      "notes": "Benchmark run with 2-shot learning"
    },
    "details": {
      "precision": 0.05128205128205128,
      "recall": 0.029411764705882353,
      "f1_score": 0.037383177570093455,
      "by_type": {
        "capitalization": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 6
        },
        "punctuation": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 50
        },
        "insertion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        },
        "replacement": {
          "precision": 0.3,
          "recall": 0.09999999999999999,
          "f1": 0.15,
          "count": 12
        },
        "deletion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        }
      },
      "correct_count": 2,
      "total_ground_truth": 68,
      "total_predicted": 39,
      "file_results": [
        {
          "model_name": "or_qwen2_5_vl_72b_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:36:00.940879",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "or_qwen2_5_vl_72b_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:36:15.644767",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_qwen2_5_vl_72b_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:36:29.391387",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "qwen/qwen2.5-vl-72b-instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T02:47:12.557462",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 4,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_qwen2_5_vl_72b_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:36:39.340855",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 4,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_qwen2_5_vl_72b_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:36:49.914874",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 4,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "qwen/qwen2.5-vl-72b-instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T02:47:29.602984",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              },
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 4,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "006"
        },
        {
          "model_name": "or_qwen2_5_vl_72b_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:37:01.302259",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 4,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "006"
        },
        {
          "model_name": "or_qwen2_5_vl_72b_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:37:12.065446",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 4,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "006"
        },
        {
          "model_name": "qwen/qwen2.5-vl-72b-instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T02:47:43.578688",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_qwen2_5_vl_72b_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:37:24.125574",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_qwen2_5_vl_72b_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:37:34.717688",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "qwen/qwen2.5-vl-72b-instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T02:48:00.209802",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        },
        {
          "model_name": "or_qwen2_5_vl_72b_instruct",
          "precision": 0.3,
          "recall": 0.075,
          "f1_score": 0.12,
          "date": "2025-04-06T08:37:55.799014",
          "details": {
            "precision": 0.3,
            "recall": 0.075,
            "f1_score": 0.12,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "replacement": {
                "precision": 0.6,
                "recall": 0.19999999999999998,
                "f1": 0.3,
                "count": 3
              }
            },
            "correct_count": 1,
            "total_ground_truth": 8,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly identifies the same edit type as the ground truth ('replacement'). The original text ('sodden') and corrected text ('soddened') match exactly between the ground truth and prediction. Line numbers and page identifiers were explicitly ignored from consideration, thus differences in those fields do not impact correctness. Since both the edit type and the core textual change are accurately captured, the prediction is correct.",
                "is_correct_with_penalty": true,
                "score": 0.6,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "replacement",
                  "original_text": "sodden",
                  "corrected_text": "soddened",
                  "line_number": 14,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "sodden",
                  "corrected_text": "soddened",
                  "line_number": 16,
                  "page": "137",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        },
        {
          "model_name": "or_qwen2_5_vl_72b_instruct",
          "precision": 0.3,
          "recall": 0.075,
          "f1_score": 0.12,
          "date": "2025-04-06T08:38:23.817647",
          "details": {
            "precision": 0.3,
            "recall": 0.075,
            "f1_score": 0.12,
            "by_type": {
              "replacement": {
                "precision": 0.6,
                "recall": 0.19999999999999998,
                "f1": 0.3,
                "count": 3
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              }
            },
            "correct_count": 1,
            "total_ground_truth": 8,
            "total_predicted": 2,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly identifies the exact edit type as 'replacement', matching exactly with the ground truth edit type. The original ('sodden') and the corrected text ('soddened') in the prediction precisely align with those in the ground truth. Additional context or different page numbers are not relevant to our focused evaluation criteria. Thus, the prediction successfully captures the intended editorial correction.",
                "is_correct_with_penalty": true,
                "score": 0.6,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "replacement",
                  "original_text": "sodden",
                  "corrected_text": "soddened",
                  "line_number": 14,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "replacement",
                  "original_text": "sodden",
                  "corrected_text": "soddened",
                  "line_number": 16,
                  "page": "137",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        }
      ]
    }
  },
  {
    "model_name": "Llama 4 Scout",
    "precision": 0.015384615384615385,
    "recall": 0.012345679012345678,
    "f1_score": 0.0136986301369863,
    "date": "2025-04-06T08:20:57.796012",
    "shots": 2,
    "config": {
      "model_id": "or_llama_4_scout",
      "display_name": "Llama 4 Scout",
      "shots": 2,
      "temperature": 0.0,
      "date": "2025-04-06",
      "notes": "Benchmark run with 2-shot learning"
    },
    "details": {
      "precision": 0.015384615384615385,
      "recall": 0.012345679012345678,
      "f1_score": 0.0136986301369863,
      "by_type": {
        "punctuation": {
          "precision": 0.02,
          "recall": 0.015000000000000001,
          "f1": 0.017142857142857144,
          "count": 60
        },
        "replacement": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 12
        },
        "capitalization": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 9
        },
        "insertion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        },
        "reordering": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        },
        "deletion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        }
      },
      "correct_count": 1,
      "total_ground_truth": 81,
      "total_predicted": 65,
      "file_results": [
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:31:12.069995",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:17:57.081895",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 4,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.3,
          "recall": 0.18,
          "f1_score": 0.225,
          "date": "2025-04-06T08:18:21.839562",
          "details": {
            "precision": 0.3,
            "recall": 0.18,
            "f1_score": 0.225,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.3,
                "recall": 0.225,
                "f1": 0.2571428571428572,
                "count": 4
              }
            },
            "correct_count": 1,
            "total_ground_truth": 5,
            "total_predicted": 3,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the ground truth across both criteria: (1) the edit type 'punctuation' matches exactly, and (2) the text content ('pockets and' corrected to 'pockets, and') accurately captures the core intention of the ground truth edit. The slight contextual difference in page identifiers or line numbers was explicitly ignored per instruction, so these differences do not affect accuracy judgment.",
                "is_correct_with_penalty": true,
                "score": 0.9,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "pockets and",
                  "corrected_text": "pockets, and",
                  "line_number": 17,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "pockets and",
                  "corrected_text": "pockets, and",
                  "line_number": 16,
                  "page": "page_19",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "003"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:31:31.534614",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:18:36.434584",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:18:49.878836",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:31:52.626380",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:18:59.887783",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:19:09.476499",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:32:24.005353",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              },
              "reordering": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 5,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The prediction correctly identifies the edit type as 'punctuation', matching the ground truth. However, the prediction indicates no actual change in text ('high-road' to 'high-road'), whereas the ground truth clearly shows a correction from 'high road' to 'high-road' (adding a hyphen). Since the key text content correction ('high road' to 'high-road') is entirely missed in the predicted edit, this prediction does not accurately capture the intended change defined by the ground truth.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "high road",
                  "corrected_text": "high-road",
                  "line_number": 1,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "high-road",
                  "corrected_text": "high-road",
                  "line_number": 2,
                  "page": "page 352",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "006"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:19:47.987466",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 6,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The edit type ('punctuation') matches correctly. However, the text content is not correctly captured. The ground truth edit is 'about and' becoming 'about, and', indicating a comma insertion after 'about'. The prediction, however, shows 'and' changed to 'and,', which incorrectly identifies the word to modify and does not accurately match the intended correction in the ground truth. Therefore, although the edit type matches, the core text change is inaccurately captured, leading to an overall incorrect prediction.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.9,
                "line_diff": 3
              },
              {
                "is_correct": false,
                "reasoning": "The edit type ('punctuation') matches exactly between the ground truth and prediction. However, the text content accuracy is incorrect. The ground truth indicates adding a comma in the phrase ('said when' \u2192 'said, when'), but the prediction incorrectly suggests removing a comma ('(comma)' \u2192 ''). Thus, although the edit type matches, the essential content of the edit (adding rather than removing punctuation) is not correctly captured in the predicted edit.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.1,
                "line_diff": 1
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "about and",
                  "corrected_text": "about, and",
                  "line_number": 13,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "and",
                  "corrected_text": "and,",
                  "line_number": 16,
                  "page": "page 352",
                  "confidence": null,
                  "notes": null
                }
              ],
              [
                {
                  "type": "punctuation",
                  "original_text": "said when",
                  "corrected_text": "said, when",
                  "line_number": 23,
                  "page": "006.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "(comma)",
                  "corrected_text": "",
                  "line_number": 22,
                  "page": "page 352",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "006"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:20:02.042111",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 12,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "006"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:32:41.765874",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:20:17.724820",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:20:32.346666",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:33:03.641555",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 4,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:20:46.887782",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        },
        {
          "model_name": "or_llama_4_scout",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:20:57.796012",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 6,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        }
      ]
    }
  },
  {
    "model_name": "Google: Gemini 2.0 Flash",
    "precision": 0.0,
    "recall": 0.0,
    "f1_score": 0,
    "date": "2025-04-05T12:15:37.937558",
    "shots": 2,
    "config": {
      "model_id": "or_gemini_2_0_flash_001",
      "display_name": "Google: Gemini 2.0 Flash",
      "shots": 2,
      "temperature": 0.0,
      "date": "2025-04-05",
      "notes": "Benchmark run with 2-shot learning"
    },
    "details": {
      "precision": 0.0,
      "recall": 0.0,
      "f1_score": 0,
      "by_type": {
        "punctuation": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 20
        },
        "capitalization": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 3
        },
        "replacement": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 4
        }
      },
      "correct_count": 0,
      "total_ground_truth": 27,
      "total_predicted": 7,
      "file_results": [
        {
          "model_name": "or_gemini_2_0_flash_001",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T12:13:36.786124",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 1,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "or_gemini_2_0_flash_001",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T12:13:58.162406",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_gemini_2_0_flash_001",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T12:14:19.804569",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_gemini_2_0_flash_001",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T12:14:43.775789",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "006"
        },
        {
          "model_name": "or_gemini_2_0_flash_001",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T12:15:06.902319",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 1,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_gemini_2_0_flash_001",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T12:15:37.937558",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 1,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The predicted edit has the correct edit type ('punctuation'), matching the ground truth. However, it misses part of the intended punctuation change. The ground truth specifies a change from 'poplar trees' to 'poplar-trees,' which includes both a hyphen and a comma. The prediction only includes the hyphen ('poplar-trees') and omits the comma. Since punctuation is integral to the correctness, the predicted edit does not fully capture the ground truth edit.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.0,
                "line_diff": 0
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees,",
                  "line_number": 6,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "poplar trees",
                  "corrected_text": "poplar-trees",
                  "line_number": 6,
                  "page": "137.jpg",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        }
      ]
    }
  },
  {
    "model_name": "Gemini 2.5 Pro Preview 3/25",
    "precision": 0.0,
    "recall": 0.0,
    "f1_score": 0,
    "date": "2025-04-06T08:47:15.966051",
    "shots": 2,
    "config": {
      "model_id": "or_gemini_2_5_pro_preview_03_25",
      "display_name": "Gemini 2.5 Pro Preview 3/25",
      "shots": 2,
      "temperature": 0.0,
      "date": "2025-04-06",
      "notes": "Benchmark run with 2-shot learning"
    },
    "details": {
      "precision": 0.0,
      "recall": 0.0,
      "f1_score": 0,
      "by_type": {
        "capitalization": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 8
        },
        "punctuation": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 48
        },
        "insertion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        },
        "deletion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        },
        "replacement": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 12
        }
      },
      "correct_count": 0,
      "total_ground_truth": 68,
      "total_predicted": 811,
      "file_results": [
        {
          "model_name": "google/gemini-2.5-pro-preview-03-25",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T03:09:51.904059",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 47,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "or_gemini_2_5_pro_preview_03_25",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:45:15.067277",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 44,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "google/gemini-2.5-pro-preview-03-25",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T03:10:11.087552",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 94,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_gemini_2_5_pro_preview_03_25",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:45:32.188352",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 65,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_gemini_2_5_pro_preview_03_25",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:45:47.015389",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 56,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "google/gemini-2.5-pro-preview-03-25",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T03:10:32.284773",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 51,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_gemini_2_5_pro_preview_03_25",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:45:56.756132",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 57,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_gemini_2_5_pro_preview_03_25",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:46:06.888397",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 3,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "google/gemini-2.5-pro-preview-03-25",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T03:10:54.003323",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 94,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "006"
        },
        {
          "model_name": "or_gemini_2_5_pro_preview_03_25",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:46:16.491941",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 50,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "006"
        },
        {
          "model_name": "google/gemini-2.5-pro-preview-03-25",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T03:11:09.806641",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 48,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_gemini_2_5_pro_preview_03_25",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:46:28.688518",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 9,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_gemini_2_5_pro_preview_03_25",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:46:45.469643",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "google/gemini-2.5-pro-preview-03-25",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T03:11:23.891358",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 48,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        },
        {
          "model_name": "or_gemini_2_5_pro_preview_03_25",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:47:00.823252",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "insertion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 85,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        },
        {
          "model_name": "or_gemini_2_5_pro_preview_03_25",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:47:15.966051",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 58,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        }
      ]
    }
  },
  {
    "model_name": "Llama 3.2 90B Vision Instruct",
    "precision": 0.0,
    "recall": 0.0,
    "f1_score": 0,
    "date": "2025-04-06T08:19:39.634894",
    "shots": 2,
    "config": {
      "model_id": "or_llama_3_2_90b_vision_instruct",
      "display_name": "Llama 3.2 90B Vision Instruct",
      "shots": 2,
      "temperature": 0.0,
      "date": "2025-04-06",
      "notes": "Benchmark run with 2-shot learning"
    },
    "details": {
      "precision": 0.0,
      "recall": 0.0,
      "f1_score": 0,
      "by_type": {
        "punctuation": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 52
        },
        "capitalization": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 7
        },
        "replacement": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 8
        },
        "deletion": {
          "precision": 0,
          "recall": 0,
          "f1": 0,
          "count": 0
        }
      },
      "correct_count": 0,
      "total_ground_truth": 67,
      "total_predicted": 56,
      "file_results": [
        {
          "model_name": "or_llama_3_2_90b_vision_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:16:57.630206",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "or_llama_3_2_90b_vision_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:18:05.362839",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 17,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "or_llama_3_2_90b_vision_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:18:35.459146",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 17,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The predicted edit correctly identifies the edit type as 'punctuation', matching the ground truth edit type exactly. However, the specific punctuation change in the original text ('pockets and' to 'pockets, and') identified by the ground truth is not correctly captured in the prediction. The predicted correction applies punctuation (a period) at the end of an entirely different section ('laugh' \u2192 'laugh.'), rather than inserting a comma between 'pockets' and 'and', which was the essential edit identified by the ground truth. Although the prediction includes the phrase 'pockets, and', it incorrectly attributes the punctuation edit elsewhere, completely missing the intended edit specified by the ground truth. Thus, the predicted edit is incorrect.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.4,
                "line_diff": 2
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "pockets and",
                  "corrected_text": "pockets, and",
                  "line_number": 17,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "The speaker, with a whimsical good humour upon him all the time, looked over the parapet-wall with the greatest disparagement of Marseilles; and taking up a determined position by putting his hands in his pockets, and rattling his money at it, apostrophised it with a short laugh",
                  "corrected_text": "The speaker, with a whimsical good humour upon him all the time, looked over the parapet-wall with the greatest disparagement of Marseilles; and taking up a determined position by putting his hands in his pockets, and rattling his money at it, apostrophised it with a short laugh.",
                  "line_number": 19,
                  "page": "001.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "003"
        },
        {
          "model_name": "or_llama_3_2_90b_vision_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:17:07.683169",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_llama_3_2_90b_vision_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:18:49.878354",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_llama_3_2_90b_vision_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:17:21.989673",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_llama_3_2_90b_vision_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:18:59.888330",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_llama_3_2_90b_vision_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:17:38.812594",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "006"
        },
        {
          "model_name": "or_llama_3_2_90b_vision_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:19:09.567112",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "006"
        },
        {
          "model_name": "or_llama_3_2_90b_vision_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:19:20.936598",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "006"
        },
        {
          "model_name": "or_llama_3_2_90b_vision_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:17:53.811611",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_llama_3_2_90b_vision_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:19:30.495577",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "deletion": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 20,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_llama_3_2_90b_vision_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:18:11.810467",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        },
        {
          "model_name": "or_llama_3_2_90b_vision_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:19:39.634894",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        }
      ]
    }
  },
  {
    "model_name": "Phi 4 Multimodal Instruct",
    "precision": 0.0,
    "recall": 0.0,
    "f1_score": 0,
    "date": "2025-04-06T08:46:33.620587",
    "shots": 2,
    "config": {
      "model_id": "or_phi_4_multimodal_instruct",
      "display_name": "Phi 4 Multimodal Instruct",
      "shots": 2,
      "temperature": 0.0,
      "date": "2025-04-06",
      "notes": "Benchmark run with 2-shot learning"
    },
    "details": {
      "precision": 0.0,
      "recall": 0.0,
      "f1_score": 0,
      "by_type": {
        "capitalization": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 8
        },
        "punctuation": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 29
        },
        "replacement": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 9
        }
      },
      "correct_count": 0,
      "total_ground_truth": 46,
      "total_predicted": 37,
      "file_results": [
        {
          "model_name": "or_phi_4_multimodal_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:17:49.890687",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "or_phi_4_multimodal_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:45:09.754277",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "or_phi_4_multimodal_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:45:25.440945",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "or_phi_4_multimodal_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:18:08.645822",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 9,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_phi_4_multimodal_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:45:37.456539",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 9,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_phi_4_multimodal_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:18:25.107330",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 1,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_phi_4_multimodal_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:45:47.015097",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 1,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_phi_4_multimodal_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:45:56.755895",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 1,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_phi_4_multimodal_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:18:41.297526",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 9,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_phi_4_multimodal_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:46:06.888766",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 7,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_phi_4_multimodal_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:46:16.492222",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_phi_4_multimodal_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T20:18:53.671959",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        },
        {
          "model_name": "or_phi_4_multimodal_instruct",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:46:33.620587",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "008"
        }
      ]
    }
  },
  {
    "model_name": "Qwen VL Plus",
    "precision": 0.0,
    "recall": 0.0,
    "f1_score": 0,
    "date": "2025-04-06T08:38:53.080839",
    "shots": 2,
    "config": {
      "model_id": "or_qwen_vl_plus",
      "display_name": "Qwen VL Plus",
      "shots": 2,
      "temperature": 0.0,
      "date": "2025-04-06",
      "notes": "Benchmark run with 2-shot learning"
    },
    "details": {
      "precision": 0.0,
      "recall": 0.0,
      "f1_score": 0,
      "by_type": {
        "punctuation": {
          "precision": 0.0010869565217391302,
          "recall": 0.0021739130434782605,
          "f1": 0.0014492753623188402,
          "count": 46
        },
        "capitalization": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 6
        },
        "replacement": {
          "precision": 0.0,
          "recall": 0.0,
          "f1": 0.0,
          "count": 11
        }
      },
      "correct_count": 0,
      "total_ground_truth": 63,
      "total_predicted": 32,
      "file_results": [
        {
          "model_name": "or_qwen_vl_plus",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:36:01.439944",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "or_qwen_vl_plus",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:36:15.798354",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 4
              }
            },
            "correct_count": 0,
            "total_ground_truth": 5,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "003"
        },
        {
          "model_name": "or_qwen_vl_plus",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:36:29.482758",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_qwen_vl_plus",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:36:39.344932",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 2
              }
            },
            "correct_count": 0,
            "total_ground_truth": 3,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "004"
        },
        {
          "model_name": "or_qwen_vl_plus",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:36:49.913870",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_qwen_vl_plus",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:37:01.301803",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 2,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "005"
        },
        {
          "model_name": "or_qwen_vl_plus",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:37:12.062853",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "006"
        },
        {
          "model_name": "or_qwen_vl_plus",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:37:24.124206",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 8
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "006"
        },
        {
          "model_name": "qwen/qwen-vl-plus",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-05T02:57:53.033574",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              },
              "capitalization": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 0
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 2,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_qwen_vl_plus",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:37:34.718208",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "or_qwen_vl_plus",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:37:51.710630",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 1
              }
            },
            "correct_count": 0,
            "total_ground_truth": 1,
            "total_predicted": 0,
            "judgments": [],
            "true_positives": []
          },
          "file_id": "007"
        },
        {
          "model_name": "qwen/qwen-vl-plus",
          "precision": 0.009999999999999998,
          "recall": 0.012499999999999997,
          "f1_score": 0.011111111111111108,
          "date": "2025-04-05T02:58:19.723586",
          "details": {
            "precision": 0.009999999999999998,
            "recall": 0.012499999999999997,
            "f1_score": 0.011111111111111108,
            "by_type": {
              "punctuation": {
                "precision": 0.009999999999999998,
                "recall": 0.019999999999999997,
                "f1": 0.01333333333333333,
                "count": 5
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 10,
            "judgments": [
              {
                "is_correct": true,
                "reasoning": "The predicted edit correctly matches the ground truth edit in edit type ('punctuation') exactly. Additionally, it correctly captures the core textual punctuation change ('man slowly' \u2192 'man, slowly'). Although the prediction includes additional surrounding context ('One ... moving on towards Chalons, was the only visible figure in the landscape'), it clearly demonstrates the exact punctuation correction intended by the ground truth. Thus, both criteria (type and core textual accuracy) are fully satisfied.",
                "is_correct_with_penalty": false,
                "score": 0.09999999999999998,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "man slowly",
                  "corrected_text": "man, slowly",
                  "line_number": 9,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "One man, slowly moving on towards Chalons, was the only visible figure in the landscape",
                  "corrected_text": "One man, slowly moving on towards Chalons, was the only visible figure in the landscape",
                  "line_number": 6,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        },
        {
          "model_name": "or_qwen_vl_plus",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:38:23.053673",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 10,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The predicted edit matches the ground truth in terms of edit type ('punctuation'), but completely fails to capture the core textual correction. The ground truth specifically indicates inserting a comma between 'man' and 'slowly' ('man slowly' to 'man, slowly'). However, the predicted edit shows no actual punctuation change; the original and corrected text given by the prediction remain identical ('One man slowly moving on towards Chalons, was the only visible figure in the landscape'). Because the core textual correction (insertion of a comma) is not represented in the prediction at all, the prediction is incorrect.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "man slowly",
                  "corrected_text": "man, slowly",
                  "line_number": 9,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "One man slowly moving on towards Chalons, was the only visible figure in the landscape",
                  "corrected_text": "One man slowly moving on towards Chalons, was the only visible figure in the landscape",
                  "line_number": 6,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        },
        {
          "model_name": "or_qwen_vl_plus",
          "precision": 0.0,
          "recall": 0.0,
          "f1_score": 0.0,
          "date": "2025-04-06T08:38:53.080839",
          "details": {
            "precision": 0.0,
            "recall": 0.0,
            "f1_score": 0.0,
            "by_type": {
              "punctuation": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 5
              },
              "replacement": {
                "precision": 0.0,
                "recall": 0.0,
                "f1": 0.0,
                "count": 3
              }
            },
            "correct_count": 0,
            "total_ground_truth": 8,
            "total_predicted": 10,
            "judgments": [
              {
                "is_correct": false,
                "reasoning": "The edit type matches ('punctuation' for both ground truth and prediction). However, the prediction does not correctly capture the essential punctuation change from the ground truth. The ground truth explicitly identifies a change from 'man slowly' to 'man, slowly', inserting a comma after 'man'. In contrast, the prediction shows the sentence 'One man slowly moving on towards Chalons, was the only visible figure in the landscape', which does not reflect the insertion of the comma directly after 'man'. Instead, the prediction places a comma much later in the sentence before 'was'. Therefore, the prediction does not accurately reflect the essential punctuation change identified in the ground truth edit.",
                "is_correct_with_penalty": false,
                "score": 0.0,
                "line_number_penalty": 0.9,
                "line_diff": 3
              }
            ],
            "true_positives": [
              [
                {
                  "type": "punctuation",
                  "original_text": "man slowly",
                  "corrected_text": "man, slowly",
                  "line_number": 9,
                  "page": "008.png",
                  "confidence": null,
                  "notes": null
                },
                {
                  "type": "punctuation",
                  "original_text": "One man slowly moving on towards Chalons, was the only visible figure in the landscape",
                  "corrected_text": "One man slowly moving on towards Chalons, was the only visible figure in the landscape",
                  "line_number": 6,
                  "page": "003.png",
                  "confidence": null,
                  "notes": null
                }
              ]
            ]
          },
          "file_id": "008"
        }
      ]
    }
  }
]